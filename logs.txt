
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND   ‚îÇ                                    ARGS                                    ‚îÇ PROFILE  ‚îÇ   USER   ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 22:20 -05 ‚îÇ 22 Dec 25 22:21 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 22:21 -05 ‚îÇ 22 Dec 25 22:21 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 22:23 -05 ‚îÇ 22 Dec 25 22:23 -05 ‚îÇ
‚îÇ docker-env ‚îÇ -u                                                                         ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 22:25 -05 ‚îÇ 22 Dec 25 22:25 -05 ‚îÇ
‚îÇ image      ‚îÇ load docker-php-phpmyadmin-apache-local:latest                             ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 22:28 -05 ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:29 -05 ‚îÇ                     ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:32 -05 ‚îÇ 24 Dec 25 12:32 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:36 -05 ‚îÇ                     ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:42 -05 ‚îÇ                     ‚îÇ
‚îÇ delete     ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:46 -05 ‚îÇ 24 Dec 25 12:46 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:47 -05 ‚îÇ 24 Dec 25 12:47 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:48 -05 ‚îÇ 24 Dec 25 12:48 -05 ‚îÇ
‚îÇ docker-env ‚îÇ -u                                                                         ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:49 -05 ‚îÇ 24 Dec 25 12:49 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:51 -05 ‚îÇ 24 Dec 25 12:51 -05 ‚îÇ
‚îÇ docker-env ‚îÇ -u                                                                         ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 12:56 -05 ‚îÇ 24 Dec 25 12:56 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable storage-provisioner                                                 ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 14:19 -05 ‚îÇ 24 Dec 25 14:19 -05 ‚îÇ
‚îÇ delete     ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 14:20 -05 ‚îÇ 24 Dec 25 14:20 -05 ‚îÇ
‚îÇ start      ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 14:20 -05 ‚îÇ 24 Dec 25 14:21 -05 ‚îÇ
‚îÇ ip         ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 14:54 -05 ‚îÇ 24 Dec 25 14:54 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 15:03 -05 ‚îÇ 24 Dec 25 15:03 -05 ‚îÇ
‚îÇ docker-env ‚îÇ -u                                                                         ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 24 Dec 25 15:21 -05 ‚îÇ 24 Dec 25 15:21 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 27 Dec 25 15:53 -05 ‚îÇ 27 Dec 25 15:53 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable storage-provisioner                                                 ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 27 Dec 25 15:53 -05 ‚îÇ 27 Dec 25 15:53 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:25 -05 ‚îÇ 08 Jan 26 19:28 -05 ‚îÇ
‚îÇ options    ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:35 -05 ‚îÇ 08 Jan 26 19:35 -05 ‚îÇ
‚îÇ addons     ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:36 -05 ‚îÇ 08 Jan 26 19:36 -05 ‚îÇ
‚îÇ addons     ‚îÇ list                                                                       ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:37 -05 ‚îÇ 08 Jan 26 19:37 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable registry                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:38 -05 ‚îÇ 08 Jan 26 19:40 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:42 -05 ‚îÇ 08 Jan 26 19:42 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 19:43 -05 ‚îÇ 08 Jan 26 19:43 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 20:21 -05 ‚îÇ 08 Jan 26 20:21 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 20:26 -05 ‚îÇ 08 Jan 26 20:26 -05 ‚îÇ
‚îÇ dashboard  ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 20:41 -05 ‚îÇ                     ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 20:41 -05 ‚îÇ 08 Jan 26 20:41 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 20:41 -05 ‚îÇ 08 Jan 26 20:41 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 20:56 -05 ‚îÇ 08 Jan 26 20:57 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 08 Jan 26 21:01 -05 ‚îÇ 08 Jan 26 21:02 -05 ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 08:12 -05 ‚îÇ 09 Jan 26 08:13 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable registry                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 08:13 -05 ‚îÇ 09 Jan 26 08:13 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 08:14 -05 ‚îÇ 09 Jan 26 08:14 -05 ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 08:14 -05 ‚îÇ 09 Jan 26 08:14 -05 ‚îÇ
‚îÇ addons     ‚îÇ list                                                                       ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 09:47 -05 ‚îÇ 09 Jan 26 09:47 -05 ‚îÇ
‚îÇ mount      ‚îÇ /home/epayco21/Escritorio/github/docker-php-phpmyadmin-apache/www:/mnt/www ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 11:20 -05 ‚îÇ                     ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 11:22 -05 ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 11:22 -05 ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 11:23 -05 ‚îÇ 09 Jan 26 11:23 -05 ‚îÇ
‚îÇ dashboard  ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 11:46 -05 ‚îÇ                     ‚îÇ
‚îÇ dashboard  ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 09 Jan 26 11:47 -05 ‚îÇ                     ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 09:01 -05 ‚îÇ                     ‚îÇ
‚îÇ start      ‚îÇ --driver=docker                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 09:01 -05 ‚îÇ 14 Jan 26 09:03 -05 ‚îÇ
‚îÇ addons     ‚îÇ list                                                                       ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 09:14 -05 ‚îÇ 14 Jan 26 09:14 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 09:15 -05 ‚îÇ 14 Jan 26 09:15 -05 ‚îÇ
‚îÇ config     ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 09:18 -05 ‚îÇ 14 Jan 26 09:18 -05 ‚îÇ
‚îÇ config     ‚îÇ get-context                                                                ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 09:35 -05 ‚îÇ 14 Jan 26 09:35 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 12:39 -05 ‚îÇ 14 Jan 26 12:39 -05 ‚îÇ
‚îÇ docker-env ‚îÇ -u                                                                         ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 17:21 -05 ‚îÇ 14 Jan 26 17:21 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 14 Jan 26 17:21 -05 ‚îÇ 14 Jan 26 17:21 -05 ‚îÇ
‚îÇ docker-env ‚îÇ                                                                            ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 15 Jan 26 17:09 -05 ‚îÇ 15 Jan 26 17:09 -05 ‚îÇ
‚îÇ service    ‚îÇ dev-stack-local --url                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 15 Jan 26 17:15 -05 ‚îÇ                     ‚îÇ
‚îÇ service    ‚îÇ dev-stack-local --url                                                      ‚îÇ minikube ‚îÇ epayco21 ‚îÇ v1.37.0 ‚îÇ 15 Jan 26 17:16 -05 ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2026/01/14 09:01:25
Running on machine: epayco21-Vostro-3400
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0114 09:01:25.751768 1619185 out.go:360] Setting OutFile to fd 1 ...
I0114 09:01:25.752129 1619185 out.go:413] isatty.IsTerminal(1) = true
I0114 09:01:25.752135 1619185 out.go:374] Setting ErrFile to fd 2...
I0114 09:01:25.752141 1619185 out.go:413] isatty.IsTerminal(2) = true
I0114 09:01:25.753475 1619185 root.go:338] Updating PATH: /home/epayco21/.minikube/bin
I0114 09:01:25.754437 1619185 out.go:368] Setting JSON to false
I0114 09:01:25.756916 1619185 start.go:130] hostinfo: {"hostname":"epayco21-Vostro-3400","uptime":91010,"bootTime":1768308276,"procs":474,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.14.0-37-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"19c18afe-4820-465e-8f04-ad0b58caf619"}
I0114 09:01:25.756992 1619185 start.go:140] virtualization: kvm host
I0114 09:01:25.777125 1619185 out.go:179] üòÑ  minikube v1.37.0 en Ubuntu 24.04
I0114 09:01:25.805693 1619185 notify.go:220] Checking for updates...
I0114 09:01:25.806174 1619185 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0114 09:01:25.809011 1619185 driver.go:421] Setting default libvirt URI to qemu:///system
I0114 09:01:25.845927 1619185 docker.go:123] docker version: linux-28.3.2:Docker Engine - Community
I0114 09:01:25.846049 1619185 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0114 09:01:25.970942 1619185 info.go:266] docker info: {ID:8672bce3-6289-4ed8-9551-508b5abe0718 Containers:5 ContainersRunning:5 ContainersPaused:0 ContainersStopped:0 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:94 OomKillDisable:false NGoroutines:217 SystemTime:2026-01-14 09:01:25.953692974 -0500 -05 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-37-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:33371336704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:epayco21-Vostro-3400 Labels:[] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:8027bnthb9pi0r0fjiswgbnax NodeAddr:192.168.1.10 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.1.10:2377 NodeID:8027bnthb9pi0r0fjiswgbnax]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2]] Warnings:<nil>}}
I0114 09:01:25.971080 1619185 docker.go:318] overlay module found
I0114 09:01:25.984527 1619185 out.go:179] ‚ú®  Using the docker driver based on existing profile
I0114 09:01:26.015773 1619185 start.go:304] selected driver: docker
I0114 09:01:26.015846 1619185 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0114 09:01:26.015947 1619185 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0114 09:01:26.016052 1619185 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0114 09:01:26.102325 1619185 info.go:266] docker info: {ID:8672bce3-6289-4ed8-9551-508b5abe0718 Containers:5 ContainersRunning:5 ContainersPaused:0 ContainersStopped:0 Images:13 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:94 OomKillDisable:false NGoroutines:217 SystemTime:2026-01-14 09:01:26.083520099 -0500 -05 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-37-generic OperatingSystem:Ubuntu 24.04.2 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:33371336704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:epayco21-Vostro-3400 Labels:[] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:8027bnthb9pi0r0fjiswgbnax NodeAddr:192.168.1.10 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.1.10:2377 NodeID:8027bnthb9pi0r0fjiswgbnax]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2]] Warnings:<nil>}}
I0114 09:01:26.103565 1619185 cni.go:84] Creating CNI manager for ""
I0114 09:01:26.103637 1619185 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0114 09:01:26.103711 1619185 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0114 09:01:26.124468 1619185 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0114 09:01:26.153836 1619185 cache.go:123] Beginning downloading kic base image for docker with docker
I0114 09:01:26.184034 1619185 out.go:179] üöú  Pulling base image v0.0.48 ...
I0114 09:01:26.218191 1619185 image.go:81] Checking for docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I0114 09:01:26.218269 1619185 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0114 09:01:26.218353 1619185 preload.go:146] Found local preload: /home/epayco21/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0114 09:01:26.218360 1619185 cache.go:58] Caching tarball of preloaded images
I0114 09:01:26.218561 1619185 preload.go:172] Found /home/epayco21/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0114 09:01:26.218579 1619185 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0114 09:01:26.218729 1619185 profile.go:143] Saving config to /home/epayco21/.minikube/profiles/minikube/config.json ...
I0114 09:01:26.274105 1619185 image.go:100] Found docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I0114 09:01:26.274116 1619185 cache.go:147] docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I0114 09:01:26.274127 1619185 cache.go:232] Successfully downloaded all kic artifacts
I0114 09:01:26.274157 1619185 start.go:360] acquireMachinesLock for minikube: {Name:mk4bdc6fefdf18fb7f4f952f8c661e1b1f367571 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0114 09:01:26.274272 1619185 start.go:364] duration metric: took 99.755¬µs to acquireMachinesLock for "minikube"
I0114 09:01:26.274292 1619185 start.go:96] Skipping create...Using existing machine configuration
I0114 09:01:26.274296 1619185 fix.go:54] fixHost starting: 
I0114 09:01:26.274571 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:26.291795 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:26.291862 1619185 fix.go:112] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:26.291878 1619185 fix.go:117] machineExists: false. err=machine does not exist
I0114 09:01:26.312449 1619185 out.go:179] ü§∑  docker "minikube" container is missing, will recreate.
I0114 09:01:26.329766 1619185 delete.go:124] DEMOLISHING minikube ...
I0114 09:01:26.329866 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:26.348787 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0114 09:01:26.348850 1619185 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:26.348873 1619185 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:26.349376 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:26.370638 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:26.370713 1619185 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:26.370792 1619185 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0114 09:01:26.386805 1619185 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0114 09:01:26.386824 1619185 kic.go:371] could not find the container minikube to remove it. will try anyways
I0114 09:01:26.386862 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:26.401313 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0114 09:01:26.401373 1619185 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:26.401445 1619185 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0114 09:01:26.416261 1619185 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0114 09:01:26.416282 1619185 oci.go:659] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0114 09:01:27.416567 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:27.433148 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:27.433198 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:27.433211 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:27.433239 1619185 retry.go:31] will retry after 488.117287ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:27.921510 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:27.945762 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:27.945820 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:27.945827 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:27.945857 1619185 retry.go:31] will retry after 464.676082ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:28.411518 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:28.429809 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:28.429878 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:28.429887 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:28.429916 1619185 retry.go:31] will retry after 1.48598518s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:29.916635 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:29.945970 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:29.946052 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:29.946062 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:29.946102 1619185 retry.go:31] will retry after 2.517438733s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:32.464605 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:32.490115 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:32.490164 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:32.490169 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:32.490190 1619185 retry.go:31] will retry after 2.698426172s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:35.190475 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:35.207107 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:35.207161 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:35.207168 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:35.207193 1619185 retry.go:31] will retry after 2.912372656s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:38.120521 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:38.143191 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:38.143250 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:38.143257 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:38.143286 1619185 retry.go:31] will retry after 7.351563911s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:45.498519 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:01:45.518895 1619185 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0114 09:01:45.518942 1619185 oci.go:671] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0114 09:01:45.518948 1619185 oci.go:673] temporary error: container minikube status is  but expect it to be exited
I0114 09:01:45.518974 1619185 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0114 09:01:45.519010 1619185 cli_runner.go:164] Run: docker rm -f -v minikube
I0114 09:01:45.534350 1619185 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0114 09:01:45.549915 1619185 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0114 09:01:45.550005 1619185 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0114 09:01:45.572191 1619185 cli_runner.go:164] Run: docker network rm minikube
I0114 09:01:45.751690 1619185 fix.go:124] Sleeping 1 second for extra luck!
I0114 09:01:46.752504 1619185 start.go:125] createHost starting for "" (driver="docker")
I0114 09:01:46.755459 1619185 out.go:252] üî•  Creating docker container (CPUs=2, Memory=7900MB) ...
I0114 09:01:46.755913 1619185 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0114 09:01:46.755938 1619185 client.go:168] LocalClient.Create starting
I0114 09:01:46.757548 1619185 main.go:141] libmachine: Reading certificate data from /home/epayco21/.minikube/certs/ca.pem
I0114 09:01:46.757941 1619185 main.go:141] libmachine: Decoding PEM data...
I0114 09:01:46.757996 1619185 main.go:141] libmachine: Parsing certificate...
I0114 09:01:46.758105 1619185 main.go:141] libmachine: Reading certificate data from /home/epayco21/.minikube/certs/cert.pem
I0114 09:01:46.758337 1619185 main.go:141] libmachine: Decoding PEM data...
I0114 09:01:46.758352 1619185 main.go:141] libmachine: Parsing certificate...
I0114 09:01:46.759074 1619185 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0114 09:01:46.788317 1619185 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0114 09:01:46.788380 1619185 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0114 09:01:46.788406 1619185 cli_runner.go:164] Run: docker network inspect minikube
W0114 09:01:46.807751 1619185 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0114 09:01:46.807775 1619185 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0114 09:01:46.807797 1619185 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0114 09:01:46.807910 1619185 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0114 09:01:46.847117 1619185 network.go:209] skipping subnet 192.168.49.0/24 that is reserved: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:<nil>}
I0114 09:01:46.849967 1619185 network.go:206] using free private subnet 192.168.58.0/24: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0007b5440}
I0114 09:01:46.849999 1619185 network_create.go:124] attempt to create docker network minikube 192.168.58.0/24 with gateway 192.168.58.1 and MTU of 1500 ...
I0114 09:01:46.850496 1619185 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0114 09:01:46.986619 1619185 network_create.go:108] docker network minikube 192.168.58.0/24 created
I0114 09:01:46.986655 1619185 kic.go:121] calculated static IP "192.168.58.2" for the "minikube" container
I0114 09:01:46.986895 1619185 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0114 09:01:47.028367 1619185 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0114 09:01:47.053922 1619185 oci.go:103] Successfully created a docker volume minikube
I0114 09:01:47.054014 1619185 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I0114 09:01:47.548359 1619185 oci.go:107] Successfully prepared a docker volume minikube
I0114 09:01:47.548402 1619185 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0114 09:01:47.548424 1619185 kic.go:194] Starting extracting preloaded images to volume ...
I0114 09:01:47.548483 1619185 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/epayco21/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I0114 09:01:52.443231 1619185 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/epayco21/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (4.894685954s)
I0114 09:01:52.443257 1619185 kic.go:203] duration metric: took 4.894830272s to extract preloaded images to volume ...
W0114 09:01:52.443424 1619185 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0114 09:01:52.443473 1619185 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0114 09:01:52.443515 1619185 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0114 09:01:52.547904 1619185 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I0114 09:01:53.964235 1619185 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.58.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7900mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1: (1.416273538s)
I0114 09:01:53.964314 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0114 09:01:53.994916 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:01:54.023089 1619185 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0114 09:01:54.107950 1619185 oci.go:144] the created container "minikube" has a running status.
I0114 09:01:54.107974 1619185 kic.go:225] Creating ssh key for kic: /home/epayco21/.minikube/machines/minikube/id_rsa...
I0114 09:01:54.744136 1619185 kic_runner.go:191] docker (temp): /home/epayco21/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0114 09:01:54.801213 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:01:54.826611 1619185 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0114 09:01:54.826624 1619185 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0114 09:01:54.963366 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:01:55.030309 1619185 machine.go:93] provisionDockerMachine start ...
I0114 09:01:55.030455 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:55.073211 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:01:55.073568 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:01:55.073578 1619185 main.go:141] libmachine: About to run SSH command:
hostname
I0114 09:01:55.304099 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0114 09:01:55.304116 1619185 ubuntu.go:182] provisioning hostname "minikube"
I0114 09:01:55.304175 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:55.339324 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:01:55.339680 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:01:55.339711 1619185 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0114 09:01:55.656067 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0114 09:01:55.656166 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:55.737749 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:01:55.738061 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:01:55.738083 1619185 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0114 09:01:55.944830 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0114 09:01:55.944849 1619185 ubuntu.go:188] set auth options {CertDir:/home/epayco21/.minikube CaCertPath:/home/epayco21/.minikube/certs/ca.pem CaPrivateKeyPath:/home/epayco21/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/epayco21/.minikube/machines/server.pem ServerKeyPath:/home/epayco21/.minikube/machines/server-key.pem ClientKeyPath:/home/epayco21/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/epayco21/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/epayco21/.minikube}
I0114 09:01:55.944892 1619185 ubuntu.go:190] setting up certificates
I0114 09:01:55.944901 1619185 provision.go:84] configureAuth start
I0114 09:01:55.944957 1619185 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0114 09:01:55.991776 1619185 provision.go:143] copyHostCerts
I0114 09:01:55.991867 1619185 exec_runner.go:144] found /home/epayco21/.minikube/ca.pem, removing ...
I0114 09:01:55.991876 1619185 exec_runner.go:203] rm: /home/epayco21/.minikube/ca.pem
I0114 09:01:55.992499 1619185 exec_runner.go:151] cp: /home/epayco21/.minikube/certs/ca.pem --> /home/epayco21/.minikube/ca.pem (1082 bytes)
I0114 09:01:55.992716 1619185 exec_runner.go:144] found /home/epayco21/.minikube/cert.pem, removing ...
I0114 09:01:55.992724 1619185 exec_runner.go:203] rm: /home/epayco21/.minikube/cert.pem
I0114 09:01:55.992986 1619185 exec_runner.go:151] cp: /home/epayco21/.minikube/certs/cert.pem --> /home/epayco21/.minikube/cert.pem (1127 bytes)
I0114 09:01:55.993122 1619185 exec_runner.go:144] found /home/epayco21/.minikube/key.pem, removing ...
I0114 09:01:55.993127 1619185 exec_runner.go:203] rm: /home/epayco21/.minikube/key.pem
I0114 09:01:55.993168 1619185 exec_runner.go:151] cp: /home/epayco21/.minikube/certs/key.pem --> /home/epayco21/.minikube/key.pem (1675 bytes)
I0114 09:01:55.993787 1619185 provision.go:117] generating server cert: /home/epayco21/.minikube/machines/server.pem ca-key=/home/epayco21/.minikube/certs/ca.pem private-key=/home/epayco21/.minikube/certs/ca-key.pem org=epayco21.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0114 09:01:56.188748 1619185 provision.go:177] copyRemoteCerts
I0114 09:01:56.188817 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0114 09:01:56.188861 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:56.327749 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:01:56.516419 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0114 09:01:56.591999 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0114 09:01:56.671795 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0114 09:01:56.735480 1619185 provision.go:87] duration metric: took 790.565854ms to configureAuth
I0114 09:01:56.735501 1619185 ubuntu.go:206] setting minikube options for container-runtime
I0114 09:01:56.735713 1619185 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0114 09:01:56.735766 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:56.776634 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:01:56.776950 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:01:56.776961 1619185 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0114 09:01:56.990945 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0114 09:01:56.990959 1619185 ubuntu.go:71] root file system type: overlay
I0114 09:01:56.991077 1619185 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0114 09:01:56.991145 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:57.021298 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:01:57.021606 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:01:57.021703 1619185 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0114 09:01:57.235953 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0114 09:01:57.236022 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:01:57.264893 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:01:57.265199 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:01:57.265220 1619185 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0114 09:02:11.076726 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-01-14 14:01:57.230207791 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0114 09:02:11.076756 1619185 machine.go:96] duration metric: took 16.04642328s to provisionDockerMachine
I0114 09:02:11.076769 1619185 client.go:171] duration metric: took 24.320825413s to LocalClient.Create
I0114 09:02:11.076790 1619185 start.go:167] duration metric: took 24.320880966s to libmachine.API.Create "minikube"
I0114 09:02:11.076797 1619185 start.go:293] postStartSetup for "minikube" (driver="docker")
I0114 09:02:11.076807 1619185 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0114 09:02:11.076893 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0114 09:02:11.076943 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:11.125381 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:11.300401 1619185 ssh_runner.go:195] Run: cat /etc/os-release
I0114 09:02:11.307095 1619185 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0114 09:02:11.307126 1619185 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0114 09:02:11.307139 1619185 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0114 09:02:11.307146 1619185 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0114 09:02:11.307167 1619185 filesync.go:126] Scanning /home/epayco21/.minikube/addons for local assets ...
I0114 09:02:11.307562 1619185 filesync.go:126] Scanning /home/epayco21/.minikube/files for local assets ...
I0114 09:02:11.307761 1619185 start.go:296] duration metric: took 230.958386ms for postStartSetup
I0114 09:02:11.310914 1619185 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0114 09:02:11.354160 1619185 profile.go:143] Saving config to /home/epayco21/.minikube/profiles/minikube/config.json ...
I0114 09:02:11.355228 1619185 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0114 09:02:11.355340 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:11.386931 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:11.523652 1619185 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0114 09:02:11.543128 1619185 start.go:128] duration metric: took 24.78583023s to createHost
I0114 09:02:11.543234 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0114 09:02:11.571640 1619185 fix.go:138] unexpected machine state, will restart: <nil>
I0114 09:02:11.571683 1619185 machine.go:93] provisionDockerMachine start ...
I0114 09:02:11.571784 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:11.613575 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:02:11.613894 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:02:11.613901 1619185 main.go:141] libmachine: About to run SSH command:
hostname
I0114 09:02:11.790831 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0114 09:02:11.790856 1619185 ubuntu.go:182] provisioning hostname "minikube"
I0114 09:02:11.790934 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:11.817309 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:02:11.817663 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:02:11.817674 1619185 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0114 09:02:11.998211 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0114 09:02:11.998327 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:12.031884 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:02:12.032211 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:02:12.032229 1619185 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0114 09:02:12.195028 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0114 09:02:12.195051 1619185 ubuntu.go:188] set auth options {CertDir:/home/epayco21/.minikube CaCertPath:/home/epayco21/.minikube/certs/ca.pem CaPrivateKeyPath:/home/epayco21/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/epayco21/.minikube/machines/server.pem ServerKeyPath:/home/epayco21/.minikube/machines/server-key.pem ClientKeyPath:/home/epayco21/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/epayco21/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/epayco21/.minikube}
I0114 09:02:12.195080 1619185 ubuntu.go:190] setting up certificates
I0114 09:02:12.195090 1619185 provision.go:84] configureAuth start
I0114 09:02:12.195145 1619185 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0114 09:02:12.239568 1619185 provision.go:143] copyHostCerts
I0114 09:02:12.239652 1619185 exec_runner.go:144] found /home/epayco21/.minikube/ca.pem, removing ...
I0114 09:02:12.239662 1619185 exec_runner.go:203] rm: /home/epayco21/.minikube/ca.pem
I0114 09:02:12.240006 1619185 exec_runner.go:151] cp: /home/epayco21/.minikube/certs/ca.pem --> /home/epayco21/.minikube/ca.pem (1082 bytes)
I0114 09:02:12.241997 1619185 exec_runner.go:144] found /home/epayco21/.minikube/cert.pem, removing ...
I0114 09:02:12.242014 1619185 exec_runner.go:203] rm: /home/epayco21/.minikube/cert.pem
I0114 09:02:12.242779 1619185 exec_runner.go:151] cp: /home/epayco21/.minikube/certs/cert.pem --> /home/epayco21/.minikube/cert.pem (1127 bytes)
I0114 09:02:12.242983 1619185 exec_runner.go:144] found /home/epayco21/.minikube/key.pem, removing ...
I0114 09:02:12.242990 1619185 exec_runner.go:203] rm: /home/epayco21/.minikube/key.pem
I0114 09:02:12.243051 1619185 exec_runner.go:151] cp: /home/epayco21/.minikube/certs/key.pem --> /home/epayco21/.minikube/key.pem (1675 bytes)
I0114 09:02:12.244267 1619185 provision.go:117] generating server cert: /home/epayco21/.minikube/machines/server.pem ca-key=/home/epayco21/.minikube/certs/ca.pem private-key=/home/epayco21/.minikube/certs/ca-key.pem org=epayco21.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0114 09:02:12.727948 1619185 provision.go:177] copyRemoteCerts
I0114 09:02:12.728006 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0114 09:02:12.728046 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:12.745969 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:12.858729 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0114 09:02:12.903239 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0114 09:02:12.986253 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0114 09:02:13.035585 1619185 provision.go:87] duration metric: took 840.478816ms to configureAuth
I0114 09:02:13.035610 1619185 ubuntu.go:206] setting minikube options for container-runtime
I0114 09:02:13.035859 1619185 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0114 09:02:13.035921 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.059416 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:02:13.059737 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:02:13.059748 1619185 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0114 09:02:13.260478 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0114 09:02:13.260522 1619185 ubuntu.go:71] root file system type: overlay
I0114 09:02:13.260654 1619185 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0114 09:02:13.260720 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.294655 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:02:13.294967 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:02:13.295063 1619185 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0114 09:02:13.485638 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0114 09:02:13.485719 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.508702 1619185 main.go:141] libmachine: Using SSH client type: native
I0114 09:02:13.509108 1619185 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0114 09:02:13.509132 1619185 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0114 09:02:13.668210 1619185 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0114 09:02:13.668238 1619185 machine.go:96] duration metric: took 2.096538882s to provisionDockerMachine
I0114 09:02:13.668250 1619185 start.go:293] postStartSetup for "minikube" (driver="docker")
I0114 09:02:13.668261 1619185 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0114 09:02:13.668328 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0114 09:02:13.668367 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.688752 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:13.798718 1619185 ssh_runner.go:195] Run: cat /etc/os-release
I0114 09:02:13.803605 1619185 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0114 09:02:13.803632 1619185 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0114 09:02:13.803643 1619185 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0114 09:02:13.803649 1619185 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0114 09:02:13.803662 1619185 filesync.go:126] Scanning /home/epayco21/.minikube/addons for local assets ...
I0114 09:02:13.803752 1619185 filesync.go:126] Scanning /home/epayco21/.minikube/files for local assets ...
I0114 09:02:13.803784 1619185 start.go:296] duration metric: took 135.525809ms for postStartSetup
I0114 09:02:13.803842 1619185 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0114 09:02:13.803940 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.821372 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:13.921864 1619185 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0114 09:02:13.926890 1619185 fix.go:56] duration metric: took 47.652581579s for fixHost
I0114 09:02:13.926906 1619185 start.go:83] releasing machines lock for "minikube", held for 47.652625925s
I0114 09:02:13.926967 1619185 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0114 09:02:13.942896 1619185 ssh_runner.go:195] Run: cat /version.json
I0114 09:02:13.942934 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.942961 1619185 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0114 09:02:13.943013 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:02:13.981895 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:13.988444 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:02:14.863520 1619185 ssh_runner.go:195] Run: systemctl --version
I0114 09:02:14.872921 1619185 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0114 09:02:14.890826 1619185 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0114 09:02:15.175569 1619185 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0114 09:02:15.175637 1619185 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0114 09:02:15.334934 1619185 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0114 09:02:15.334966 1619185 start.go:495] detecting cgroup driver to use...
I0114 09:02:15.335005 1619185 detect.go:190] detected "systemd" cgroup driver on host os
I0114 09:02:15.335134 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0114 09:02:15.385804 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0114 09:02:15.482275 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0114 09:02:15.523309 1619185 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0114 09:02:15.523414 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0114 09:02:15.542249 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0114 09:02:15.573808 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0114 09:02:15.614837 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0114 09:02:15.647279 1619185 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0114 09:02:15.671608 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0114 09:02:15.720763 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0114 09:02:15.765967 1619185 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0114 09:02:15.827953 1619185 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0114 09:02:15.859429 1619185 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0114 09:02:15.880767 1619185 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0114 09:02:16.082962 1619185 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0114 09:02:16.427439 1619185 start.go:495] detecting cgroup driver to use...
I0114 09:02:16.427499 1619185 detect.go:190] detected "systemd" cgroup driver on host os
I0114 09:02:16.427563 1619185 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0114 09:02:16.448670 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0114 09:02:16.477027 1619185 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0114 09:02:16.502971 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0114 09:02:16.528724 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0114 09:02:16.559448 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0114 09:02:16.600643 1619185 ssh_runner.go:195] Run: which cri-dockerd
I0114 09:02:16.608567 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0114 09:02:16.644739 1619185 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0114 09:02:16.691523 1619185 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0114 09:02:16.877850 1619185 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0114 09:02:17.053813 1619185 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I0114 09:02:17.053927 1619185 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0114 09:02:17.094044 1619185 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0114 09:02:17.116497 1619185 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0114 09:02:17.242074 1619185 ssh_runner.go:195] Run: sudo systemctl restart docker
I0114 09:02:47.787561 1619185 ssh_runner.go:235] Completed: sudo systemctl restart docker: (30.545460897s)
I0114 09:02:47.787697 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0114 09:02:47.848147 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0114 09:02:47.949460 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0114 09:02:47.979901 1619185 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0114 09:02:48.317768 1619185 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0114 09:02:48.837538 1619185 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0114 09:02:49.317403 1619185 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0114 09:02:49.609647 1619185 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0114 09:02:49.640258 1619185 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0114 09:02:50.013859 1619185 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0114 09:02:50.682746 1619185 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0114 09:02:50.731145 1619185 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0114 09:02:50.731211 1619185 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0114 09:02:50.742094 1619185 start.go:563] Will wait 60s for crictl version
I0114 09:02:50.742158 1619185 ssh_runner.go:195] Run: which crictl
I0114 09:02:50.748710 1619185 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0114 09:02:51.267768 1619185 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I0114 09:02:51.267832 1619185 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0114 09:02:51.488974 1619185 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0114 09:02:51.564297 1619185 out.go:252] üê≥  Preparando Kubernetes v1.34.0 en Docker 28.4.0...
I0114 09:02:51.564476 1619185 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0114 09:02:51.623509 1619185 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0114 09:02:51.636300 1619185 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0114 09:02:51.663449 1619185 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0114 09:02:51.663591 1619185 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0114 09:02:51.663654 1619185 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0114 09:02:51.703803 1619185 docker.go:691] Got preloaded images: -- stdout --
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
<none>:<none>
registry.k8s.io/pause:3.10.1
<none>:<none>
<none>:<none>
registry.k8s.io/coredns/coredns:v1.12.1
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0114 09:02:51.703816 1619185 docker.go:621] Images already preloaded, skipping extraction
I0114 09:02:51.703894 1619185 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0114 09:02:51.774044 1619185 docker.go:691] Got preloaded images: -- stdout --
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
<none>:<none>
registry.k8s.io/pause:3.10.1
<none>:<none>
<none>:<none>
registry.k8s.io/coredns/coredns:v1.12.1
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0114 09:02:51.774080 1619185 cache_images.go:85] Images are preloaded, skipping loading
I0114 09:02:51.774090 1619185 kubeadm.go:926] updating node { 192.168.58.2 8443 v1.34.0 docker true true} ...
I0114 09:02:51.774198 1619185 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0114 09:02:51.774255 1619185 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0114 09:02:52.204627 1619185 cni.go:84] Creating CNI manager for ""
I0114 09:02:52.204652 1619185 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0114 09:02:52.204663 1619185 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0114 09:02:52.204687 1619185 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0114 09:02:52.204841 1619185 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.58.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0114 09:02:52.204901 1619185 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0114 09:02:52.225292 1619185 binaries.go:44] Found k8s binaries, skipping transfer
I0114 09:02:52.225365 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0114 09:02:52.243137 1619185 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0114 09:02:52.297721 1619185 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0114 09:02:52.368095 1619185 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0114 09:02:52.445741 1619185 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0114 09:02:52.463660 1619185 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0114 09:02:52.497649 1619185 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0114 09:02:52.774047 1619185 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0114 09:02:52.885775 1619185 certs.go:68] Setting up /home/epayco21/.minikube/profiles/minikube for IP: 192.168.58.2
I0114 09:02:52.885786 1619185 certs.go:194] generating shared ca certs ...
I0114 09:02:52.885804 1619185 certs.go:226] acquiring lock for ca certs: {Name:mk6ee9d7aae4900a5301297ac6851403c96e0343 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0114 09:02:52.886735 1619185 certs.go:235] skipping valid "minikubeCA" ca cert: /home/epayco21/.minikube/ca.key
I0114 09:02:52.887171 1619185 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/epayco21/.minikube/proxy-client-ca.key
I0114 09:02:52.887190 1619185 certs.go:256] generating profile certs ...
I0114 09:02:52.887310 1619185 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/epayco21/.minikube/profiles/minikube/client.key
I0114 09:02:52.887567 1619185 certs.go:363] generating signed profile cert for "minikube": /home/epayco21/.minikube/profiles/minikube/apiserver.key.502bbb95
I0114 09:02:52.887588 1619185 crypto.go:68] Generating cert /home/epayco21/.minikube/profiles/minikube/apiserver.crt.502bbb95 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.58.2]
I0114 09:02:53.432027 1619185 crypto.go:156] Writing cert to /home/epayco21/.minikube/profiles/minikube/apiserver.crt.502bbb95 ...
I0114 09:02:53.432050 1619185 lock.go:35] WriteFile acquiring /home/epayco21/.minikube/profiles/minikube/apiserver.crt.502bbb95: {Name:mk2518ee0cd464c39d74b6cdc280dda4ffe86c3f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0114 09:02:53.433951 1619185 crypto.go:164] Writing key to /home/epayco21/.minikube/profiles/minikube/apiserver.key.502bbb95 ...
I0114 09:02:53.433975 1619185 lock.go:35] WriteFile acquiring /home/epayco21/.minikube/profiles/minikube/apiserver.key.502bbb95: {Name:mk0910495f4f8bdb46be4d8f9e704cfb6efe3116 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0114 09:02:53.434175 1619185 certs.go:381] copying /home/epayco21/.minikube/profiles/minikube/apiserver.crt.502bbb95 -> /home/epayco21/.minikube/profiles/minikube/apiserver.crt
I0114 09:02:53.434410 1619185 certs.go:385] copying /home/epayco21/.minikube/profiles/minikube/apiserver.key.502bbb95 -> /home/epayco21/.minikube/profiles/minikube/apiserver.key
I0114 09:02:53.434950 1619185 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/epayco21/.minikube/profiles/minikube/proxy-client.key
I0114 09:02:53.435295 1619185 certs.go:484] found cert: /home/epayco21/.minikube/certs/ca-key.pem (1675 bytes)
I0114 09:02:53.435564 1619185 certs.go:484] found cert: /home/epayco21/.minikube/certs/ca.pem (1082 bytes)
I0114 09:02:53.435778 1619185 certs.go:484] found cert: /home/epayco21/.minikube/certs/cert.pem (1127 bytes)
I0114 09:02:53.435985 1619185 certs.go:484] found cert: /home/epayco21/.minikube/certs/key.pem (1675 bytes)
I0114 09:02:53.437217 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0114 09:02:53.655653 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0114 09:02:53.803145 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0114 09:02:53.912233 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0114 09:02:54.076969 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0114 09:02:54.203534 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0114 09:02:54.374252 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0114 09:02:54.469417 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0114 09:02:54.552136 1619185 ssh_runner.go:362] scp /home/epayco21/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0114 09:02:54.653064 1619185 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0114 09:02:54.759086 1619185 ssh_runner.go:195] Run: openssl version
I0114 09:02:54.784114 1619185 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0114 09:02:54.825908 1619185 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0114 09:02:54.834871 1619185 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 18 18:52 /usr/share/ca-certificates/minikubeCA.pem
I0114 09:02:54.834921 1619185 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0114 09:02:54.864845 1619185 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0114 09:02:54.918497 1619185 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0114 09:02:54.927529 1619185 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0114 09:02:54.956018 1619185 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0114 09:02:54.986695 1619185 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0114 09:02:55.005195 1619185 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0114 09:02:55.034613 1619185 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0114 09:02:55.090611 1619185 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0114 09:02:55.109822 1619185 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:7900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:true registry:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0114 09:02:55.109986 1619185 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0114 09:02:55.174993 1619185 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0114 09:02:55.205595 1619185 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0114 09:02:55.205606 1619185 kubeadm.go:589] restartPrimaryControlPlane start ...
I0114 09:02:55.205655 1619185 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0114 09:02:55.227173 1619185 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0114 09:02:55.233954 1619185 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I0114 09:02:55.233978 1619185 kubeconfig.go:47] verify endpoint returned: got: 192.168.49.2:8443, want: 192.168.58.2:8443
I0114 09:02:55.240960 1619185 kubeconfig.go:62] /home/epayco21/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0114 09:02:55.244509 1619185 lock.go:35] WriteFile acquiring /home/epayco21/.kube/config: {Name:mkcebcd53c37a69aea31ea8b33d969d75cf73b22 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0114 09:02:55.252871 1619185 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0114 09:02:55.317739 1619185 kubeadm.go:636] detected kubeadm config drift (will reconfigure cluster from new /var/tmp/minikube/kubeadm.yaml):
-- stdout --
--- /var/tmp/minikube/kubeadm.yaml	2026-01-09 00:27:20.503265777 +0000
+++ /var/tmp/minikube/kubeadm.yaml.new	2026-01-14 14:02:52.436039953 +0000
@@ -1,7 +1,7 @@
 apiVersion: kubeadm.k8s.io/v1beta4
 kind: InitConfiguration
 localAPIEndpoint:
-  advertiseAddress: 192.168.49.2
+  advertiseAddress: 192.168.58.2
   bindPort: 8443
 bootstrapTokens:
   - groups:
@@ -15,13 +15,13 @@
   name: "minikube"
   kubeletExtraArgs:
     - name: "node-ip"
-      value: "192.168.49.2"
+      value: "192.168.58.2"
   taints: []
 ---
 apiVersion: kubeadm.k8s.io/v1beta4
 kind: ClusterConfiguration
 apiServer:
-  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
+  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
   extraArgs:
     - name: "enable-admission-plugins"
       value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"

-- /stdout --
I0114 09:02:55.317762 1619185 kubeadm.go:1152] stopping kube-system containers ...
I0114 09:02:55.317832 1619185 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0114 09:02:55.417356 1619185 docker.go:484] Stopping containers: [cb2a5b723259 bab2f45ecab1 30dfca714698 0359eff05f98 92b90437bc6f ae86b21bb747 706e5a6a5f9e db012239a5c7 d98f17594623 01932defb182 5100b8cdf233 ca8e96acbaff 277508c78ac7 7099658f2c95 df558151195c e0d9f491b432 b9b8e7fa4543 c4dee48061bf 539eae43833d 470a27314b0e 63b607a15b75 3d9335dde7bf 03f04c4ac544 56513a0c5d40 fb379d2d1153 4e6700fda7cd 691e0d3f3581 7ac422b8ec44 ce4d1c087101 2a660e4177aa fddd77a51f7e 6e17998b9eee 0d698e868953 be71c8ee535d 0ef89aa7719e d17b54230089 4d58beb6083c bebbbae9f747 93eb7227cd92 5e2303814676 4aa8ebb41f95 5f4fe5e43559 6ef112b1b544]
I0114 09:02:55.417455 1619185 ssh_runner.go:195] Run: docker stop cb2a5b723259 bab2f45ecab1 30dfca714698 0359eff05f98 92b90437bc6f ae86b21bb747 706e5a6a5f9e db012239a5c7 d98f17594623 01932defb182 5100b8cdf233 ca8e96acbaff 277508c78ac7 7099658f2c95 df558151195c e0d9f491b432 b9b8e7fa4543 c4dee48061bf 539eae43833d 470a27314b0e 63b607a15b75 3d9335dde7bf 03f04c4ac544 56513a0c5d40 fb379d2d1153 4e6700fda7cd 691e0d3f3581 7ac422b8ec44 ce4d1c087101 2a660e4177aa fddd77a51f7e 6e17998b9eee 0d698e868953 be71c8ee535d 0ef89aa7719e d17b54230089 4d58beb6083c bebbbae9f747 93eb7227cd92 5e2303814676 4aa8ebb41f95 5f4fe5e43559 6ef112b1b544
I0114 09:02:55.474588 1619185 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0114 09:02:55.529577 1619185 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0114 09:02:55.564614 1619185 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0114 09:02:55.564626 1619185 kubeadm.go:157] found existing configuration files:

I0114 09:02:55.564679 1619185 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0114 09:02:55.588442 1619185 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0114 09:02:55.588491 1619185 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0114 09:02:55.633622 1619185 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0114 09:02:55.668205 1619185 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0114 09:02:55.668266 1619185 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0114 09:02:55.718256 1619185 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0114 09:02:55.747702 1619185 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0114 09:02:55.747755 1619185 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0114 09:02:55.770544 1619185 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0114 09:02:55.798730 1619185 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0114 09:02:55.798786 1619185 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0114 09:02:55.821254 1619185 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0114 09:02:55.846247 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0114 09:02:56.154708 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0114 09:02:57.647001 1619185 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.492267104s)
I0114 09:02:57.647022 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0114 09:02:58.065910 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0114 09:02:58.269233 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0114 09:02:58.393424 1619185 api_server.go:52] waiting for apiserver process to appear ...
I0114 09:02:58.393507 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:02:58.893582 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:02:59.393567 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:02:59.893683 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:03:00.394330 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:03:00.893731 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:03:01.396503 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:03:01.657055 1619185 api_server.go:72] duration metric: took 3.263666223s to wait for apiserver process to appear ...
I0114 09:03:01.657072 1619185 api_server.go:88] waiting for apiserver healthz status ...
I0114 09:03:01.657092 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:02.167666 1619185 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I0114 09:03:02.167701 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:02.167996 1619185 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I0114 09:03:02.657704 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:06.747423 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0114 09:03:06.747446 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0114 09:03:06.747464 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:06.768133 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0114 09:03:06.768150 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0114 09:03:07.157541 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:07.166092 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:07.166114 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:07.657728 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:07.661250 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:07.661265 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:08.157930 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:08.164274 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:08.164296 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:08.657823 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:08.666749 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:08.666775 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:09.157442 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:09.163534 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:09.163554 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:09.657943 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:09.664669 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:09.664693 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:10.157663 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:10.162308 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:10.162331 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:10.657525 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:10.666409 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:10.666428 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:11.157239 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:11.162984 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:11.163011 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:11.658294 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:11.665000 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0114 09:03:11.665020 1619185 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0114 09:03:12.157548 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:12.163339 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 200:
ok
I0114 09:03:12.180414 1619185 api_server.go:141] control plane version: v1.34.0
I0114 09:03:12.180437 1619185 api_server.go:131] duration metric: took 10.523357225s to wait for apiserver health ...
I0114 09:03:12.180451 1619185 cni.go:84] Creating CNI manager for ""
I0114 09:03:12.180467 1619185 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0114 09:03:12.225946 1619185 out.go:179] üîó  Configurando CNI bridge CNI ...
I0114 09:03:12.287698 1619185 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0114 09:03:12.304926 1619185 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0114 09:03:12.344478 1619185 system_pods.go:43] waiting for kube-system pods to appear ...
I0114 09:03:12.354971 1619185 system_pods.go:59] 11 kube-system pods found
I0114 09:03:12.354997 1619185 system_pods.go:61] "coredns-66bc5c9577-dqgvb" [a820a242-3927-46d0-911a-de6849e7b5d8] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0114 09:03:12.355005 1619185 system_pods.go:61] "coredns-66bc5c9577-gr4z8" [87057d81-20c1-4b08-845c-5ac487ece071] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0114 09:03:12.355012 1619185 system_pods.go:61] "etcd-minikube" [7f3e4919-cfdf-4d63-ba9a-cf56b3ead5af] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0114 09:03:12.355018 1619185 system_pods.go:61] "kube-apiserver-minikube" [5e90de6e-43b6-4491-a3f3-9918f7b8577f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0114 09:03:12.355022 1619185 system_pods.go:61] "kube-controller-manager-minikube" [52600f2a-cf5c-4e25-b338-3d725647b5bf] Running
I0114 09:03:12.355025 1619185 system_pods.go:61] "kube-proxy-rsr5r" [b86af07f-e2ac-41f4-98cb-accdbea48557] Running
I0114 09:03:12.355029 1619185 system_pods.go:61] "kube-scheduler-minikube" [4a0b84cb-ed71-44d3-9d15-de6bf3768129] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0114 09:03:12.355034 1619185 system_pods.go:61] "metrics-server-85b7d694d7-bn56v" [c53da74e-94ad-4df4-b96c-785b174cce68] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0114 09:03:12.355038 1619185 system_pods.go:61] "registry-66898fdd98-cstcq" [45eade46-2e7a-44ab-a30e-f913143e28d4] Running / Ready:ContainersNotReady (containers with unready status: [registry]) / ContainersReady:ContainersNotReady (containers with unready status: [registry])
I0114 09:03:12.355042 1619185 system_pods.go:61] "registry-proxy-kj69m" [9304c17f-c7fd-437e-b71c-60773523b5c6] Running / Ready:ContainersNotReady (containers with unready status: [registry-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [registry-proxy])
I0114 09:03:12.355046 1619185 system_pods.go:61] "storage-provisioner" [6db460be-9810-4b42-b06a-8c74390c5687] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0114 09:03:12.355050 1619185 system_pods.go:74] duration metric: took 10.560426ms to wait for pod list to return data ...
I0114 09:03:12.355058 1619185 node_conditions.go:102] verifying NodePressure condition ...
I0114 09:03:12.361448 1619185 node_conditions.go:122] node storage ephemeral capacity is 490048472Ki
I0114 09:03:12.361477 1619185 node_conditions.go:123] node cpu capacity is 8
I0114 09:03:12.361492 1619185 node_conditions.go:105] duration metric: took 6.430334ms to run NodePressure ...
I0114 09:03:12.361510 1619185 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0114 09:03:14.887183 1619185 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.52563163s)
I0114 09:03:14.887211 1619185 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
W0114 09:03:14.915220 1619185 kubeadm.go:740] unable to adjust resource limits: oom_adj check cmd /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj". : /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": Process exited with status 1
stdout:

stderr:
cat: /proc/5280: Is a directory
cat: 6048/oom_adj: No such file or directory
I0114 09:03:14.915243 1619185 kubeadm.go:593] duration metric: took 19.709632465s to restartPrimaryControlPlane
I0114 09:03:14.915253 1619185 kubeadm.go:394] duration metric: took 19.805453539s to StartCluster
I0114 09:03:14.915275 1619185 settings.go:142] acquiring lock: {Name:mk8df3bdabd88952d71ae90acb20cb12ae66db4b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0114 09:03:14.915447 1619185 settings.go:150] Updating kubeconfig:  /home/epayco21/.kube/config
I0114 09:03:14.931747 1619185 lock.go:35] WriteFile acquiring /home/epayco21/.kube/config: {Name:mkcebcd53c37a69aea31ea8b33d969d75cf73b22 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0114 09:03:14.932401 1619185 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0114 09:03:14.932535 1619185 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:true registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0114 09:03:14.932628 1619185 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0114 09:03:14.932648 1619185 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0114 09:03:14.932656 1619185 addons.go:247] addon storage-provisioner should already be in state true
I0114 09:03:14.932684 1619185 host.go:66] Checking if "minikube" exists ...
I0114 09:03:14.932765 1619185 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0114 09:03:14.932793 1619185 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0114 09:03:14.933437 1619185 addons.go:69] Setting dashboard=true in profile "minikube"
I0114 09:03:14.933456 1619185 addons.go:238] Setting addon dashboard=true in "minikube"
W0114 09:03:14.933463 1619185 addons.go:247] addon dashboard should already be in state true
I0114 09:03:14.933499 1619185 host.go:66] Checking if "minikube" exists ...
I0114 09:03:14.934480 1619185 addons.go:69] Setting metrics-server=true in profile "minikube"
I0114 09:03:14.934508 1619185 addons.go:238] Setting addon metrics-server=true in "minikube"
W0114 09:03:14.934517 1619185 addons.go:247] addon metrics-server should already be in state true
I0114 09:03:14.934554 1619185 host.go:66] Checking if "minikube" exists ...
I0114 09:03:14.935603 1619185 addons.go:69] Setting registry=true in profile "minikube"
I0114 09:03:14.935637 1619185 addons.go:238] Setting addon registry=true in "minikube"
W0114 09:03:14.935651 1619185 addons.go:247] addon registry should already be in state true
I0114 09:03:14.935693 1619185 host.go:66] Checking if "minikube" exists ...
I0114 09:03:14.945044 1619185 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0114 09:03:14.948170 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:03:14.948174 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:03:14.948628 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:03:14.958637 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:03:14.959820 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:03:14.995678 1619185 out.go:179] üîé  Verifying Kubernetes components...
I0114 09:03:15.064011 1619185 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0114 09:03:15.064026 1619185 addons.go:247] addon default-storageclass should already be in state true
I0114 09:03:15.064057 1619185 host.go:66] Checking if "minikube" exists ...
I0114 09:03:15.064668 1619185 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0114 09:03:15.080819 1619185 out.go:179]     ‚ñ™ Using image registry.k8s.io/metrics-server/metrics-server:v0.8.0
I0114 09:03:15.080877 1619185 out.go:179]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0114 09:03:15.080979 1619185 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/kube-registry-proxy:0.0.9
I0114 09:03:15.080955 1619185 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0114 09:03:15.081078 1619185 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0114 09:03:15.096067 1619185 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0114 09:03:15.096081 1619185 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0114 09:03:15.096144 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:03:15.130947 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:03:15.183226 1619185 addons.go:435] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0114 09:03:15.183244 1619185 ssh_runner.go:362] scp metrics-server/metrics-apiservice.yaml --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0114 09:03:15.183314 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:03:15.183433 1619185 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0114 09:03:15.183677 1619185 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0114 09:03:15.183739 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:03:15.233468 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:03:15.238370 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:03:15.253198 1619185 out.go:179]     ‚ñ™ Using image docker.io/registry:3.0.0
I0114 09:03:15.253403 1619185 out.go:179]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0114 09:03:15.363963 1619185 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0114 09:03:15.392734 1619185 api_server.go:52] waiting for apiserver process to appear ...
I0114 09:03:15.392798 1619185 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0114 09:03:15.431612 1619185 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0114 09:03:15.434351 1619185 api_server.go:72] duration metric: took 501.91623ms to wait for apiserver process to appear ...
I0114 09:03:15.434391 1619185 api_server.go:88] waiting for apiserver healthz status ...
I0114 09:03:15.434411 1619185 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I0114 09:03:15.437392 1619185 addons.go:435] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0114 09:03:15.437409 1619185 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0114 09:03:15.442050 1619185 api_server.go:279] https://192.168.58.2:8443/healthz returned 200:
ok
I0114 09:03:15.443279 1619185 api_server.go:141] control plane version: v1.34.0
I0114 09:03:15.443297 1619185 api_server.go:131] duration metric: took 8.8997ms to wait for apiserver health ...
I0114 09:03:15.443305 1619185 system_pods.go:43] waiting for kube-system pods to appear ...
I0114 09:03:15.450221 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0114 09:03:15.450239 1619185 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0114 09:03:15.450310 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:03:15.451630 1619185 addons.go:435] installing /etc/kubernetes/addons/registry-rc.yaml
I0114 09:03:15.451640 1619185 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/registry-rc.yaml (860 bytes)
I0114 09:03:15.451690 1619185 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0114 09:03:15.461612 1619185 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0114 09:03:15.495633 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:03:15.541170 1619185 system_pods.go:59] 11 kube-system pods found
I0114 09:03:15.541197 1619185 system_pods.go:61] "coredns-66bc5c9577-dqgvb" [a820a242-3927-46d0-911a-de6849e7b5d8] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0114 09:03:15.541206 1619185 system_pods.go:61] "coredns-66bc5c9577-gr4z8" [87057d81-20c1-4b08-845c-5ac487ece071] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0114 09:03:15.541214 1619185 system_pods.go:61] "etcd-minikube" [7f3e4919-cfdf-4d63-ba9a-cf56b3ead5af] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0114 09:03:15.541218 1619185 system_pods.go:61] "kube-apiserver-minikube" [5e90de6e-43b6-4491-a3f3-9918f7b8577f] Running
I0114 09:03:15.541224 1619185 system_pods.go:61] "kube-controller-manager-minikube" [52600f2a-cf5c-4e25-b338-3d725647b5bf] Running
I0114 09:03:15.541227 1619185 system_pods.go:61] "kube-proxy-rsr5r" [b86af07f-e2ac-41f4-98cb-accdbea48557] Running
I0114 09:03:15.541232 1619185 system_pods.go:61] "kube-scheduler-minikube" [4a0b84cb-ed71-44d3-9d15-de6bf3768129] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0114 09:03:15.541238 1619185 system_pods.go:61] "metrics-server-85b7d694d7-bn56v" [c53da74e-94ad-4df4-b96c-785b174cce68] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0114 09:03:15.541241 1619185 system_pods.go:61] "registry-66898fdd98-cstcq" [45eade46-2e7a-44ab-a30e-f913143e28d4] Running
I0114 09:03:15.541246 1619185 system_pods.go:61] "registry-proxy-kj69m" [9304c17f-c7fd-437e-b71c-60773523b5c6] Running / Ready:ContainersNotReady (containers with unready status: [registry-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [registry-proxy])
I0114 09:03:15.541249 1619185 system_pods.go:61] "storage-provisioner" [6db460be-9810-4b42-b06a-8c74390c5687] Running
I0114 09:03:15.541256 1619185 system_pods.go:74] duration metric: took 97.945695ms to wait for pod list to return data ...
I0114 09:03:15.541273 1619185 kubeadm.go:578] duration metric: took 608.839549ms to wait for: map[apiserver:true system_pods:true]
I0114 09:03:15.541289 1619185 node_conditions.go:102] verifying NodePressure condition ...
I0114 09:03:15.543444 1619185 addons.go:435] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0114 09:03:15.543462 1619185 ssh_runner.go:362] scp metrics-server/metrics-server-rbac.yaml --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0114 09:03:15.546980 1619185 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/epayco21/.minikube/machines/minikube/id_rsa Username:docker}
I0114 09:03:15.589599 1619185 addons.go:435] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0114 09:03:15.589611 1619185 ssh_runner.go:362] scp metrics-server/metrics-server-service.yaml --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0114 09:03:15.625919 1619185 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0114 09:03:15.655648 1619185 node_conditions.go:122] node storage ephemeral capacity is 490048472Ki
I0114 09:03:15.655666 1619185 node_conditions.go:123] node cpu capacity is 8
I0114 09:03:15.655675 1619185 node_conditions.go:105] duration metric: took 114.382903ms to run NodePressure ...
I0114 09:03:15.655685 1619185 start.go:241] waiting for startup goroutines ...
I0114 09:03:15.701347 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0114 09:03:15.701358 1619185 addons.go:435] installing /etc/kubernetes/addons/registry-svc.yaml
I0114 09:03:15.701363 1619185 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0114 09:03:15.701368 1619185 ssh_runner.go:362] scp registry/registry-svc.yaml --> /etc/kubernetes/addons/registry-svc.yaml (398 bytes)
I0114 09:03:15.754286 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0114 09:03:15.754299 1619185 addons.go:435] installing /etc/kubernetes/addons/registry-proxy.yaml
I0114 09:03:15.754299 1619185 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0114 09:03:15.754304 1619185 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/registry-proxy.yaml (947 bytes)
I0114 09:03:15.802596 1619185 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/registry-rc.yaml -f /etc/kubernetes/addons/registry-svc.yaml -f /etc/kubernetes/addons/registry-proxy.yaml
I0114 09:03:15.803161 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0114 09:03:15.803173 1619185 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0114 09:03:15.839383 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0114 09:03:15.839406 1619185 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0114 09:03:15.874285 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0114 09:03:15.874318 1619185 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0114 09:03:15.922149 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0114 09:03:15.922162 1619185 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0114 09:03:16.003712 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0114 09:03:16.003729 1619185 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0114 09:03:16.074328 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0114 09:03:16.074344 1619185 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0114 09:03:16.186014 1619185 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0114 09:03:16.186029 1619185 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0114 09:03:16.237830 1619185 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0114 09:03:19.802495 1619185 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (4.37083181s)
I0114 09:03:21.081583 1619185 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.619945425s)
I0114 09:03:21.563531 1619185 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (5.937577865s)
I0114 09:03:21.563557 1619185 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/registry-rc.yaml -f /etc/kubernetes/addons/registry-svc.yaml -f /etc/kubernetes/addons/registry-proxy.yaml: (5.760944729s)
I0114 09:03:21.563559 1619185 addons.go:479] Verifying addon metrics-server=true in "minikube"
I0114 09:03:21.563567 1619185 addons.go:479] Verifying addon registry=true in "minikube"
I0114 09:03:21.626653 1619185 out.go:179] üîé  Verifying registry addon...
I0114 09:03:21.699998 1619185 kapi.go:75] Waiting for pod with label "kubernetes.io/minikube-addons=registry" in ns "kube-system" ...
I0114 09:03:21.718539 1619185 kapi.go:86] Found 2 Pods for label selector kubernetes.io/minikube-addons=registry
I0114 09:03:21.718552 1619185 kapi.go:107] duration metric: took 18.563138ms to wait for kubernetes.io/minikube-addons=registry ...
I0114 09:03:22.571781 1619185 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (6.333914126s)
I0114 09:03:22.628905 1619185 out.go:179] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0114 09:03:22.689428 1619185 out.go:179] üåü  Complementos habilitados: default-storageclass, storage-provisioner, metrics-server, registry, dashboard
I0114 09:03:22.716918 1619185 addons.go:514] duration metric: took 7.784382421s for enable addons: enabled=[default-storageclass storage-provisioner metrics-server registry dashboard]
I0114 09:03:22.716974 1619185 start.go:246] waiting for cluster config update ...
I0114 09:03:22.716986 1619185 start.go:255] writing updated cluster config ...
I0114 09:03:22.733525 1619185 ssh_runner.go:195] Run: rm -f paused
I0114 09:03:22.818304 1619185 start.go:617] kubectl: 1.33.3, cluster: 1.34.0 (minor skew: 1)
I0114 09:03:22.867964 1619185 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 15 17:26:11 minikube cri-dockerd[4560]: time="2026-01-15T17:26:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 17:31:11 minikube cri-dockerd[4560]: time="2026-01-15T17:31:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 17:36:11 minikube cri-dockerd[4560]: time="2026-01-15T17:36:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 17:41:11 minikube cri-dockerd[4560]: time="2026-01-15T17:41:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 17:46:11 minikube cri-dockerd[4560]: time="2026-01-15T17:46:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 17:51:11 minikube cri-dockerd[4560]: time="2026-01-15T17:51:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 17:56:11 minikube cri-dockerd[4560]: time="2026-01-15T17:56:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:01:11 minikube cri-dockerd[4560]: time="2026-01-15T18:01:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:06:11 minikube cri-dockerd[4560]: time="2026-01-15T18:06:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:11:11 minikube cri-dockerd[4560]: time="2026-01-15T18:11:11Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:16:12 minikube cri-dockerd[4560]: time="2026-01-15T18:16:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:21:12 minikube cri-dockerd[4560]: time="2026-01-15T18:21:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:26:12 minikube cri-dockerd[4560]: time="2026-01-15T18:26:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:31:12 minikube cri-dockerd[4560]: time="2026-01-15T18:31:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:36:12 minikube cri-dockerd[4560]: time="2026-01-15T18:36:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:41:12 minikube cri-dockerd[4560]: time="2026-01-15T18:41:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:46:12 minikube cri-dockerd[4560]: time="2026-01-15T18:46:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:51:12 minikube cri-dockerd[4560]: time="2026-01-15T18:51:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 18:56:12 minikube cri-dockerd[4560]: time="2026-01-15T18:56:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:01:12 minikube cri-dockerd[4560]: time="2026-01-15T19:01:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:06:12 minikube cri-dockerd[4560]: time="2026-01-15T19:06:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:11:12 minikube cri-dockerd[4560]: time="2026-01-15T19:11:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:16:12 minikube cri-dockerd[4560]: time="2026-01-15T19:16:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:21:12 minikube cri-dockerd[4560]: time="2026-01-15T19:21:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:26:12 minikube cri-dockerd[4560]: time="2026-01-15T19:26:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:31:12 minikube cri-dockerd[4560]: time="2026-01-15T19:31:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:36:12 minikube cri-dockerd[4560]: time="2026-01-15T19:36:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:41:12 minikube cri-dockerd[4560]: time="2026-01-15T19:41:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:46:12 minikube cri-dockerd[4560]: time="2026-01-15T19:46:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:51:12 minikube cri-dockerd[4560]: time="2026-01-15T19:51:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 19:56:12 minikube cri-dockerd[4560]: time="2026-01-15T19:56:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:01:12 minikube cri-dockerd[4560]: time="2026-01-15T20:01:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:06:12 minikube cri-dockerd[4560]: time="2026-01-15T20:06:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:11:12 minikube cri-dockerd[4560]: time="2026-01-15T20:11:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:16:12 minikube cri-dockerd[4560]: time="2026-01-15T20:16:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:21:12 minikube cri-dockerd[4560]: time="2026-01-15T20:21:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:26:12 minikube cri-dockerd[4560]: time="2026-01-15T20:26:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:31:12 minikube cri-dockerd[4560]: time="2026-01-15T20:31:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:36:12 minikube cri-dockerd[4560]: time="2026-01-15T20:36:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:41:12 minikube cri-dockerd[4560]: time="2026-01-15T20:41:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:46:12 minikube cri-dockerd[4560]: time="2026-01-15T20:46:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:51:12 minikube cri-dockerd[4560]: time="2026-01-15T20:51:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 20:56:12 minikube cri-dockerd[4560]: time="2026-01-15T20:56:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:01:12 minikube cri-dockerd[4560]: time="2026-01-15T21:01:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:06:12 minikube cri-dockerd[4560]: time="2026-01-15T21:06:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:11:12 minikube cri-dockerd[4560]: time="2026-01-15T21:11:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:16:12 minikube cri-dockerd[4560]: time="2026-01-15T21:16:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:21:12 minikube cri-dockerd[4560]: time="2026-01-15T21:21:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:26:12 minikube cri-dockerd[4560]: time="2026-01-15T21:26:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:31:12 minikube cri-dockerd[4560]: time="2026-01-15T21:31:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:36:12 minikube cri-dockerd[4560]: time="2026-01-15T21:36:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:41:12 minikube cri-dockerd[4560]: time="2026-01-15T21:41:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:46:12 minikube cri-dockerd[4560]: time="2026-01-15T21:46:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:51:12 minikube cri-dockerd[4560]: time="2026-01-15T21:51:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 21:56:12 minikube cri-dockerd[4560]: time="2026-01-15T21:56:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 22:01:12 minikube cri-dockerd[4560]: time="2026-01-15T22:01:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 22:06:12 minikube cri-dockerd[4560]: time="2026-01-15T22:06:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 22:11:12 minikube cri-dockerd[4560]: time="2026-01-15T22:11:12Z" level=info msg="pid 4459 is not running in the host namespaces"
Jan 15 22:14:33 minikube cri-dockerd[4560]: time="2026-01-15T22:14:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e755c5ff511eb1d6ad212f34c3c7e39e5b5e45aab30e75d037684a2d95a4bce6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local une.net.co options ndots:5]"
Jan 15 22:16:12 minikube cri-dockerd[4560]: time="2026-01-15T22:16:12Z" level=info msg="pid 4459 is not running in the host namespaces"


==> container status <==
CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
3eb8ce49948ae       1954d7c6807c4                                                                                                           2 minutes ago       Running             apache-local                0                   e755c5ff511eb       apache-local-pod
a5edb0584fb56       6e38f40d628db                                                                                                           32 hours ago        Running             storage-provisioner         27                  55e275ee82396       storage-provisioner
c349890fa0e9f       b1c9f9ef5f0c2                                                                                                           32 hours ago        Running             registry-proxy              7                   f99a346b630d4       registry-proxy-kj69m
22d3175147bf2       3c52eedeec804                                                                                                           32 hours ago        Running             registry                    7                   9c525b1650e7c       registry-66898fdd98-cstcq
c2e5befb147b1       115053965e86b                                                                                                           32 hours ago        Running             dashboard-metrics-scraper   7                   80d0cf5e899a7       dashboard-metrics-scraper-77bf4d6c4c-4lb8b
9273d6b03ee49       b9e1e3849e070                                                                                                           32 hours ago        Running             metrics-server              22                  9689edec52280       metrics-server-85b7d694d7-bn56v
a2ef6ef3eb808       07655ddf2eebe                                                                                                           32 hours ago        Running             kubernetes-dashboard        16                  3ebcafaa759b3       kubernetes-dashboard-855c9754f9-hzkh6
6fe798577e089       52546a367cc9e                                                                                                           32 hours ago        Running             coredns                     7                   d00939cbcca2a       coredns-66bc5c9577-gr4z8
81e31d56a3a1f       52546a367cc9e                                                                                                           32 hours ago        Running             coredns                     7                   8183a67142457       coredns-66bc5c9577-dqgvb
3206b3a4a7bb5       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c                    32 hours ago        Exited              dashboard-metrics-scraper   6                   42a6702a31f98       dashboard-metrics-scraper-77bf4d6c4c-4lb8b
1f06ad3bbf454       registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2   32 hours ago        Exited              metrics-server              21                  fe9a646be5767       metrics-server-85b7d694d7-bn56v
d8bc155c92e7f       gcr.io/k8s-minikube/kube-registry-proxy@sha256:f832bbe1d48c62de040bd793937eaa0c05d2f945a55376a99c80a4dd9961aeb1         32 hours ago        Exited              registry-proxy              6                   8239887b60c76       registry-proxy-kj69m
cc80e71e4c056       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93                          32 hours ago        Exited              kubernetes-dashboard        15                  8d37899f726a9       kubernetes-dashboard-855c9754f9-hzkh6
1d3b1f2d9750e       registry@sha256:3725021071ec9383eb3d87ddbdff9ed602439b3f7c958c9c2fb941049ea6531d                                        32 hours ago        Exited              registry                    6                   2e8fe3e4dad1c       registry-66898fdd98-cstcq
e7d61ffedaa9c       52546a367cc9e                                                                                                           32 hours ago        Exited              coredns                     6                   8d77452975dbb       coredns-66bc5c9577-gr4z8
6617e1648ab39       6e38f40d628db                                                                                                           32 hours ago        Exited              storage-provisioner         26                  55e275ee82396       storage-provisioner
b79ea78b971be       52546a367cc9e                                                                                                           32 hours ago        Exited              coredns                     6                   47da3d772dae6       coredns-66bc5c9577-dqgvb
f6c10c3200ebe       df0860106674d                                                                                                           32 hours ago        Running             kube-proxy                  16                  416469c7dd81f       kube-proxy-rsr5r
d695fca0df27c       90550c43ad2bc                                                                                                           32 hours ago        Running             kube-apiserver              0                   fd44bea5f3625       kube-apiserver-minikube
badad02a68486       a0af72f2ec6d6                                                                                                           32 hours ago        Running             kube-controller-manager     7                   4e200ec3bdc4d       kube-controller-manager-minikube
ae2ebb1895f8f       46169d968e920                                                                                                           32 hours ago        Running             kube-scheduler              6                   1196ba795a223       kube-scheduler-minikube
66136fe046d8e       5f1f5298c888d                                                                                                           32 hours ago        Running             etcd                        0                   12ce7ff09aba9       etcd-minikube
ae86b21bb747b       a0af72f2ec6d6                                                                                                           6 days ago          Exited              kube-controller-manager     6                   e0d9f491b4320       kube-controller-manager-minikube
db012239a5c70       df0860106674d                                                                                                           6 days ago          Exited              kube-proxy                  15                  c4dee48061bff       kube-proxy-rsr5r
5100b8cdf2331       46169d968e920                                                                                                           6 days ago          Exited              kube-scheduler              5                   3d9335dde7bf9       kube-scheduler-minikube


==> coredns [6fe798577e08] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> coredns [81e31d56a3a1] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1


==> coredns [b79ea78b971b] <==


==> coredns [e7d61ffedaa9] <==


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_01_08T19_28_14_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 09 Jan 2026 00:28:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 15 Jan 2026 22:16:36 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 15 Jan 2026 22:13:30 +0000   Fri, 09 Jan 2026 00:28:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 15 Jan 2026 22:13:30 +0000   Fri, 09 Jan 2026 00:28:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 15 Jan 2026 22:13:30 +0000   Fri, 09 Jan 2026 00:28:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 15 Jan 2026 22:13:30 +0000   Fri, 09 Jan 2026 00:28:07 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  490048472Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32589196Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  490048472Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32589196Ki
  pods:               110
System Info:
  Machine ID:                 193e8a41f0104d38b36ae1175031b3f2
  System UUID:                1912c09f-e8d7-47ea-a87e-694a1dc57edd
  Boot ID:                    f0b177f3-e07b-4073-af1c-cd65113cc357
  Kernel Version:             6.14.0-37-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     apache-local-pod                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m12s
  kube-system                 coredns-66bc5c9577-dqgvb                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d21h
  kube-system                 coredns-66bc5c9577-gr4z8                      100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     6d21h
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         32h
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         32h
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         6d21h
  kube-system                 kube-proxy-rsr5r                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d21h
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         6d21h
  kube-system                 metrics-server-85b7d694d7-bn56v               100m (1%)     0 (0%)      200Mi (0%)       0 (0%)         6d21h
  kube-system                 registry-66898fdd98-cstcq                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d21h
  kube-system                 registry-proxy-kj69m                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d21h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d21h
  kubernetes-dashboard        dashboard-metrics-scraper-77bf4d6c4c-4lb8b    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d20h
  kubernetes-dashboard        kubernetes-dashboard-855c9754f9-hzkh6         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d20h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (11%)  0 (0%)
  memory             440Mi (1%)  340Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[ +15.094698] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +11.995919] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Jan15 09:04] Bluetooth: hci0: SCO packet for unknown connection handle 3
[  +0.000007] Bluetooth: hci0: SCO packet for unknown connection handle 3
[Jan15 10:07] kauditd_printk_skb: 30 callbacks suppressed
[Jan15 11:36] kauditd_printk_skb: 4 callbacks suppressed
[Jan15 11:45] kauditd_printk_skb: 48 callbacks suppressed
[ +11.043846] kauditd_printk_skb: 4 callbacks suppressed
[Jan15 11:46] kauditd_printk_skb: 4 callbacks suppressed
[Jan15 12:57] Bluetooth: hci0: SCO packet for unknown connection handle 3
[  +0.000010] Bluetooth: hci0: SCO packet for unknown connection handle 3
[Jan15 13:34] Bluetooth: hci0: SCO packet for unknown connection handle 4
[  +0.000010] Bluetooth: hci0: SCO packet for unknown connection handle 4
[Jan15 13:37] kauditd_printk_skb: 2 callbacks suppressed
[Jan15 14:33] kauditd_printk_skb: 9 callbacks suppressed
[Jan15 15:30] workqueue: delayed_fput hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Jan15 15:37] kauditd_printk_skb: 11 callbacks suppressed
[Jan15 15:56] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +21.849242] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +4.599249] kauditd_printk_skb: 44 callbacks suppressed
[Jan15 15:57] kauditd_printk_skb: 8 callbacks suppressed
[  +5.997169] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Jan15 15:59] kauditd_printk_skb: 4 callbacks suppressed
[Jan15 16:27] workqueue: delayed_fput hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[Jan15 17:23] done.
[Jan15 17:37] kauditd_printk_skb: 2 callbacks suppressed
[Jan15 18:13] workqueue: delayed_fput hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Jan16 06:52] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +0.013991] workqueue: acpi_os_execute_deferred hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[  +3.365371] workqueue: acpi_ec_event_processor hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +5.297053] workqueue: acpi_ec_event_processor hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[  +0.762045] done.
[  +3.793995] kauditd_printk_skb: 47 callbacks suppressed
[Jan16 07:21] kauditd_printk_skb: 2 callbacks suppressed
[Jan16 09:04] Bluetooth: hci0: SCO packet for unknown connection handle 3
[  +0.000012] Bluetooth: hci0: SCO packet for unknown connection handle 3
[Jan16 09:22] kauditd_printk_skb: 49 callbacks suppressed
[Jan16 09:58] Bluetooth: hci0: SCO packet for unknown connection handle 4
[Jan16 11:18] kauditd_printk_skb: 93 callbacks suppressed
[Jan16 13:14] kauditd_printk_skb: 4 callbacks suppressed
[Jan16 13:25] workqueue: inode_switch_wbs_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[Jan16 13:33] kauditd_printk_skb: 48 callbacks suppressed
[Jan16 13:34] kauditd_printk_skb: 6 callbacks suppressed
[Jan16 13:35] kauditd_printk_skb: 18 callbacks suppressed
[ +10.446723] kauditd_printk_skb: 4 callbacks suppressed
[Jan16 13:39] Bluetooth: hci0: SCO packet for unknown connection handle 3
[  +0.000007] Bluetooth: hci0: SCO packet for unknown connection handle 3
[  +0.000004] Bluetooth: hci0: SCO packet for unknown connection handle 3
[Jan16 14:55] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.152242] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +5.374336] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000005] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.106164] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000006] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Jan16 15:41] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +3.959365] overlayfs: upperdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.000010] overlayfs: workdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +4.588768] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[  +0.123843] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +12.640899] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.


==> etcd [66136fe046d8] <==
{"level":"info","ts":"2026-01-15T20:41:28.489523Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":98072}
{"level":"info","ts":"2026-01-15T20:41:28.501611Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":98072,"took":"11.79129ms","hash":3580816769,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1560576,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T20:41:28.501665Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3580816769,"revision":98072,"compact-revision":97832}
{"level":"info","ts":"2026-01-15T20:46:28.500576Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":98311}
{"level":"info","ts":"2026-01-15T20:46:28.505719Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":98311,"took":"4.83897ms","hash":1659223073,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T20:46:28.505786Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1659223073,"revision":98311,"compact-revision":98072}
{"level":"info","ts":"2026-01-15T20:51:28.511050Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":98552}
{"level":"info","ts":"2026-01-15T20:51:28.515381Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":98552,"took":"4.168352ms","hash":537619019,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1560576,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T20:51:28.515413Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":537619019,"revision":98552,"compact-revision":98311}
{"level":"info","ts":"2026-01-15T20:56:28.545711Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":98791}
{"level":"info","ts":"2026-01-15T20:56:28.550245Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":98791,"took":"4.392599ms","hash":1836677089,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T20:56:28.550277Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1836677089,"revision":98791,"compact-revision":98552}
{"level":"info","ts":"2026-01-15T21:01:28.554581Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":99031}
{"level":"info","ts":"2026-01-15T21:01:28.581816Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":99031,"took":"27.025363ms","hash":440613404,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1511424,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:01:28.581855Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":440613404,"revision":99031,"compact-revision":98791}
{"level":"info","ts":"2026-01-15T21:06:28.564462Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":99272}
{"level":"info","ts":"2026-01-15T21:06:28.589605Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":99272,"took":"24.60305ms","hash":1199580947,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1531904,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:06:28.589661Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1199580947,"revision":99272,"compact-revision":99031}
{"level":"info","ts":"2026-01-15T21:11:28.591049Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":99511}
{"level":"info","ts":"2026-01-15T21:11:28.603581Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":99511,"took":"12.194997ms","hash":1631426368,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1519616,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:11:28.603643Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1631426368,"revision":99511,"compact-revision":99272}
{"level":"info","ts":"2026-01-15T21:16:28.609789Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":99752}
{"level":"info","ts":"2026-01-15T21:16:28.614762Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":99752,"took":"4.685046ms","hash":1409722325,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1515520,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:16:28.614834Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1409722325,"revision":99752,"compact-revision":99511}
{"level":"info","ts":"2026-01-15T21:21:28.622016Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":99991}
{"level":"info","ts":"2026-01-15T21:21:28.627650Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":99991,"took":"5.293456ms","hash":3404481408,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1523712,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:21:28.627735Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3404481408,"revision":99991,"compact-revision":99752}
{"level":"info","ts":"2026-01-15T21:26:28.631900Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":100230}
{"level":"info","ts":"2026-01-15T21:26:28.635835Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":100230,"took":"3.769127ms","hash":2556370510,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1507328,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:26:28.635862Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2556370510,"revision":100230,"compact-revision":99991}
{"level":"info","ts":"2026-01-15T21:31:28.641846Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":100470}
{"level":"info","ts":"2026-01-15T21:31:28.647759Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":100470,"took":"4.703661ms","hash":1084923949,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1548288,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T21:31:28.647813Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1084923949,"revision":100470,"compact-revision":100230}
{"level":"info","ts":"2026-01-15T21:36:28.651599Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":100709}
{"level":"info","ts":"2026-01-15T21:36:28.656499Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":100709,"took":"4.680659ms","hash":1150610199,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1556480,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T21:36:28.656552Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1150610199,"revision":100709,"compact-revision":100470}
{"level":"info","ts":"2026-01-15T21:41:28.664281Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":100949}
{"level":"info","ts":"2026-01-15T21:41:28.669923Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":100949,"took":"5.2982ms","hash":1540729106,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1556480,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T21:41:28.669985Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1540729106,"revision":100949,"compact-revision":100709}
{"level":"info","ts":"2026-01-15T21:46:28.690530Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":101189}
{"level":"info","ts":"2026-01-15T21:46:28.700937Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":101189,"took":"10.212329ms","hash":3597318586,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1572864,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T21:46:28.700989Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3597318586,"revision":101189,"compact-revision":100949}
{"level":"info","ts":"2026-01-15T21:51:28.703481Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":101428}
{"level":"info","ts":"2026-01-15T21:51:28.721574Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":101428,"took":"17.76364ms","hash":302658191,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T21:51:28.721634Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":302658191,"revision":101428,"compact-revision":101189}
{"level":"info","ts":"2026-01-15T21:56:28.726960Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":101668}
{"level":"info","ts":"2026-01-15T21:56:28.735547Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":101668,"took":"8.23384ms","hash":2169953666,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1560576,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T21:56:28.735601Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2169953666,"revision":101668,"compact-revision":101428}
{"level":"info","ts":"2026-01-15T22:01:28.761426Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":101908}
{"level":"info","ts":"2026-01-15T22:01:28.767327Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":101908,"took":"5.668888ms","hash":1990466050,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1548288,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T22:01:28.767383Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1990466050,"revision":101908,"compact-revision":101668}
{"level":"info","ts":"2026-01-15T22:06:28.772989Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":102148}
{"level":"info","ts":"2026-01-15T22:06:28.778241Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":102148,"took":"4.969474ms","hash":2749093325,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-01-15T22:06:28.778298Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2749093325,"revision":102148,"compact-revision":101908}
{"level":"info","ts":"2026-01-15T22:11:28.785603Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":102388}
{"level":"info","ts":"2026-01-15T22:11:28.790757Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":102388,"took":"4.92801ms","hash":2927008943,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1572864,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T22:11:28.790797Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2927008943,"revision":102388,"compact-revision":102148}
{"level":"info","ts":"2026-01-15T22:16:28.794666Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":102627}
{"level":"info","ts":"2026-01-15T22:16:28.798607Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":102627,"took":"3.769695ms","hash":1377913180,"current-db-size-bytes":4440064,"current-db-size":"4.4 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-01-15T22:16:28.798642Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1377913180,"revision":102627,"compact-revision":102388}


==> kernel <==
 22:16:44 up 2 days,  9:32,  0 users,  load average: 1.49, 1.37, 1.91
Linux minikube 6.14.0-37-generic #37~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 20 10:25:38 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [d695fca0df27] <==
I0115 21:42:06.452845       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:43:06.583154       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:43:23.569202       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:44:15.746117       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:44:29.750292       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:45:28.981636       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:45:42.116749       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:46:32.889853       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:46:47.737389       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:47:55.019051       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:48:12.614256       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:48:55.574531       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:49:29.875625       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:50:10.354340       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:50:43.712128       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:51:12.605108       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:51:25.201123       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0115 21:52:03.347623       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:52:20.656932       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:53:18.998570       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:53:39.997788       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:54:41.941509       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:55:07.336085       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:56:03.538025       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:56:33.499661       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:57:04.689135       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:57:49.262964       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:58:24.640045       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:59:14.892938       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 21:59:52.739158       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:00:24.223282       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:01:00.505641       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:01:25.201354       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0115 22:01:35.609032       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:02:04.891472       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:02:53.369930       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:03:26.176697       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:04:03.660326       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:04:48.176033       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:05:13.165599       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:06:12.438222       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:06:24.948070       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:07:26.939867       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:07:29.170385       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:08:31.495475       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:08:33.603541       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:09:38.407446       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:09:40.504116       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:10:51.654377       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:11:05.138113       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:11:25.201631       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0115 22:12:04.001752       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:12:23.875009       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:13:24.446418       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:13:31.641379       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:14:38.204144       1 alloc.go:328] "allocated clusterIPs" service="default/dev-stack-local" clusterIPs={"IPv4":"10.107.225.185"}
I0115 22:14:52.292854       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:14:55.129063       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:16:08.131167       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0115 22:16:19.903316       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [ae86b21bb747] <==
I0109 16:23:09.644839       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0109 16:23:09.648991       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0109 16:23:09.651101       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0109 16:23:09.657386       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0109 16:23:09.657669       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0109 16:23:09.658793       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0109 16:23:09.658793       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0109 16:23:09.659131       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0109 16:23:09.660986       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0109 16:23:09.660994       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0109 16:23:09.661111       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0109 16:23:09.662388       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0109 16:23:09.664574       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0109 16:23:09.667817       1 shared_informer.go:356] "Caches are synced" controller="node"
I0109 16:23:09.667858       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0109 16:23:09.667888       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0109 16:23:09.667898       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0109 16:23:09.667905       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0109 16:23:09.668985       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0109 16:23:09.674107       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0109 16:23:09.674114       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0109 16:23:09.674120       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0109 16:23:09.674115       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0109 16:23:09.677310       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0109 16:23:09.680533       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0109 16:23:09.682829       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0109 16:23:09.689027       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0109 16:23:09.689079       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0109 16:23:09.689129       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0109 16:23:09.689170       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0109 16:23:09.691246       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0109 16:23:09.695601       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0109 16:23:09.698716       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0109 16:23:09.701880       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0109 16:23:09.708657       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0109 16:23:09.708666       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0109 16:23:09.708660       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0109 16:23:09.708681       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I0109 16:23:09.708857       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0109 16:23:09.709786       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0109 16:23:09.710479       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0109 16:23:09.711621       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0109 16:23:09.711727       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0109 16:23:09.714855       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0109 16:23:09.721016       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0109 16:23:09.723136       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
E0109 16:23:39.714272       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0109 16:23:39.727112       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0109 16:24:09.717300       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0109 16:24:09.731514       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0109 18:41:24.985061       1 pv_controller_base.go:216] "Could not sync volume" err="Unauthorized" logger="persistentvolume-binder-controller" volumeName="pvc-f1156525-2274-4816-8552-bb89066c0c6d" err="Unauthorized"
E0110 13:50:39.405760       1 pv_controller_base.go:216] "Could not sync volume" err="Unauthorized" logger="persistentvolume-binder-controller" volumeName="pvc-758cf16f-39a7-406b-9b6e-d805286ab342" err="Unauthorized"
E0110 13:50:56.976316       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0110 13:50:56.977557       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0110 18:17:51.117635       1 pv_controller_base.go:216] "Could not sync volume" err="Unauthorized" logger="persistentvolume-binder-controller" volumeName="pvc-f1156525-2274-4816-8552-bb89066c0c6d" err="Unauthorized"
E0110 18:17:54.673434       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0110 18:17:54.731940       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0111 04:09:25.903995       1 pv_controller_base.go:216] "Could not sync volume" err="Unauthorized" logger="persistentvolume-binder-controller" volumeName="pvc-758cf16f-39a7-406b-9b6e-d805286ab342" err="Unauthorized"
E0111 04:09:29.772073       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0111 04:09:29.822584       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-controller-manager [badad02a6848] <==
I0114 14:03:14.751601       1 servicecidrs_controller.go:137] "Starting" logger="service-cidr-controller" controller="service-cidr-controller"
I0114 14:03:14.751618       1 shared_informer.go:349] "Waiting for caches to sync" controller="service-cidr-controller"
I0114 14:03:14.760675       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I0114 14:03:14.782423       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I0114 14:03:14.826484       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0114 14:03:14.826873       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0114 14:03:14.828849       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I0114 14:03:14.831276       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0114 14:03:14.832810       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I0114 14:03:14.835150       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I0114 14:03:14.837534       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I0114 14:03:14.838879       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0114 14:03:14.850023       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I0114 14:03:14.852442       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I0114 14:03:14.861672       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0114 14:03:14.863915       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0114 14:03:14.865726       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I0114 14:03:14.868960       1 shared_informer.go:356] "Caches are synced" controller="GC"
I0114 14:03:14.871318       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I0114 14:03:14.875461       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I0114 14:03:14.882754       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0114 14:03:14.882859       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0114 14:03:14.882946       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0114 14:03:14.883000       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0114 14:03:14.884113       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0114 14:03:14.884547       1 shared_informer.go:356] "Caches are synced" controller="service account"
I0114 14:03:14.887501       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I0114 14:03:14.887915       1 shared_informer.go:356] "Caches are synced" controller="node"
I0114 14:03:14.887960       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0114 14:03:14.888001       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0114 14:03:14.888008       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I0114 14:03:14.888016       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I0114 14:03:14.892183       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0114 14:03:14.892211       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0114 14:03:14.892222       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0114 14:03:14.900541       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I0114 14:03:14.902277       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I0114 14:03:14.903035       1 shared_informer.go:356] "Caches are synced" controller="taint"
I0114 14:03:14.903367       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0114 14:03:14.903632       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I0114 14:03:14.903743       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0114 14:03:14.904191       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0114 14:03:14.904281       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0114 14:03:14.908636       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I0114 14:03:14.908638       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I0114 14:03:14.911538       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I0114 14:03:14.914442       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I0114 14:03:14.915354       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I0114 14:03:14.916155       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I0114 14:03:14.917912       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I0114 14:03:14.919812       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I0114 14:03:14.921954       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I0114 14:03:14.923161       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I0114 14:03:14.923293       1 shared_informer.go:356] "Caches are synced" controller="expand"
I0114 14:03:14.927824       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I0114 14:03:14.929965       1 shared_informer.go:356] "Caches are synced" controller="job"
I0114 14:03:14.931040       1 shared_informer.go:356] "Caches are synced" controller="disruption"
E0115 12:57:40.106072       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0115 12:57:40.112200       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0115 12:57:49.099778       1 pv_controller_base.go:216] "Could not sync volume" err="Unauthorized" logger="persistentvolume-binder-controller" volumeName="pvc-f1156525-2274-4816-8552-bb89066c0c6d" err="Unauthorized"


==> kube-proxy [db012239a5c7] <==
I0109 16:23:02.831483       1 server_linux.go:53] "Using iptables proxy"
I0109 16:23:03.009496       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0109 16:23:04.209673       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0109 16:23:04.209708       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0109 16:23:04.209812       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0109 16:23:04.256621       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0109 16:23:04.256667       1 server_linux.go:132] "Using iptables Proxier"
I0109 16:23:04.264567       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0109 16:23:04.265002       1 server.go:527] "Version info" version="v1.34.0"
I0109 16:23:04.265047       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0109 16:23:04.266407       1 config.go:106] "Starting endpoint slice config controller"
I0109 16:23:04.266424       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0109 16:23:04.266445       1 config.go:200] "Starting service config controller"
I0109 16:23:04.266450       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0109 16:23:04.266629       1 config.go:309] "Starting node config controller"
I0109 16:23:04.267012       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0109 16:23:04.267119       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0109 16:23:04.266839       1 config.go:403] "Starting serviceCIDR config controller"
I0109 16:23:04.267204       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0109 16:23:04.367323       1 shared_informer.go:356] "Caches are synced" controller="service config"
I0109 16:23:04.367345       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0109 16:23:04.367355       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-proxy [f6c10c3200eb] <==
I0114 14:03:12.796876       1 server_linux.go:53] "Using iptables proxy"
I0114 14:03:13.299034       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0114 14:03:13.400461       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0114 14:03:13.400536       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.58.2"]
E0114 14:03:13.400648       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0114 14:03:13.973374       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0114 14:03:13.973442       1 server_linux.go:132] "Using iptables Proxier"
I0114 14:03:13.981588       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0114 14:03:14.246748       1 server.go:527] "Version info" version="v1.34.0"
I0114 14:03:14.246792       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0114 14:03:14.251010       1 config.go:200] "Starting service config controller"
I0114 14:03:14.251037       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0114 14:03:14.251074       1 config.go:106] "Starting endpoint slice config controller"
I0114 14:03:14.251079       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0114 14:03:14.251090       1 config.go:403] "Starting serviceCIDR config controller"
I0114 14:03:14.251114       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0114 14:03:14.251771       1 config.go:309] "Starting node config controller"
I0114 14:03:14.252049       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0114 14:03:14.252182       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0114 14:03:14.351149       1 shared_informer.go:356] "Caches are synced" controller="service config"
I0114 14:03:14.351690       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0114 14:03:14.351700       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [5100b8cdf233] <==
I0109 16:23:04.158982       1 serving.go:386] Generated self-signed cert in-memory
I0109 16:23:04.628953       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0109 16:23:04.628973       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0109 16:23:04.632102       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0109 16:23:04.632124       1 shared_informer.go:349] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0109 16:23:04.632152       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 16:23:04.632166       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 16:23:04.632183       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0109 16:23:04.632172       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 16:23:04.632434       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0109 16:23:04.632480       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0109 16:23:04.733078       1 shared_informer.go:356] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0109 16:23:04.733087       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0109 16:23:04.733095       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0111 04:16:18.426241       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I0111 04:16:18.429841       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I0111 04:16:18.432583       1 server.go:265] "[graceful-termination] secure server is exiting"
I0111 04:16:18.434324       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0111 04:16:18.434856       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0111 04:16:18.435056       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [ae2ebb1895f8] <==
I0114 14:03:02.638517       1 serving.go:386] Generated self-signed cert in-memory
W0114 14:03:06.761291       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0114 14:03:06.761326       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0114 14:03:06.761340       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0114 14:03:06.761349       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0114 14:03:06.807275       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0114 14:03:06.807322       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0114 14:03:06.811190       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0114 14:03:06.811922       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0114 14:03:06.811949       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0114 14:03:06.812009       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0114 14:03:06.912455       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jan 15 22:12:06 minikube kubelet[5591]: E0115 22:12:06.909963    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15521875 maxSize=10485760
Jan 15 22:12:16 minikube kubelet[5591]: E0115 22:12:16.916466    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:12:16 minikube kubelet[5591]: E0115 22:12:16.916687    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15523795 maxSize=10485760
Jan 15 22:12:26 minikube kubelet[5591]: E0115 22:12:26.923092    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:12:26 minikube kubelet[5591]: E0115 22:12:26.923333    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15525715 maxSize=10485760
Jan 15 22:12:36 minikube kubelet[5591]: E0115 22:12:36.924315    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:12:36 minikube kubelet[5591]: E0115 22:12:36.924413    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15527635 maxSize=10485760
Jan 15 22:12:46 minikube kubelet[5591]: E0115 22:12:46.932146    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:12:46 minikube kubelet[5591]: E0115 22:12:46.932360    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15529554 maxSize=10485760
Jan 15 22:12:56 minikube kubelet[5591]: E0115 22:12:56.939119    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:12:56 minikube kubelet[5591]: E0115 22:12:56.939432    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15531474 maxSize=10485760
Jan 15 22:13:06 minikube kubelet[5591]: E0115 22:13:06.945901    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:13:06 minikube kubelet[5591]: E0115 22:13:06.946108    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15533394 maxSize=10485760
Jan 15 22:13:16 minikube kubelet[5591]: E0115 22:13:16.951297    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:13:16 minikube kubelet[5591]: E0115 22:13:16.951552    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15535313 maxSize=10485760
Jan 15 22:13:26 minikube kubelet[5591]: E0115 22:13:26.958517    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:13:26 minikube kubelet[5591]: E0115 22:13:26.958747    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15537233 maxSize=10485760
Jan 15 22:13:36 minikube kubelet[5591]: E0115 22:13:36.964734    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:13:36 minikube kubelet[5591]: E0115 22:13:36.964997    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15539152 maxSize=10485760
Jan 15 22:13:46 minikube kubelet[5591]: E0115 22:13:46.965192    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:13:46 minikube kubelet[5591]: E0115 22:13:46.965272    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15541071 maxSize=10485760
Jan 15 22:13:56 minikube kubelet[5591]: E0115 22:13:56.974966    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:13:56 minikube kubelet[5591]: E0115 22:13:56.975136    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15542990 maxSize=10485760
Jan 15 22:14:06 minikube kubelet[5591]: E0115 22:14:06.981367    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:14:06 minikube kubelet[5591]: E0115 22:14:06.981536    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15544909 maxSize=10485760
Jan 15 22:14:16 minikube kubelet[5591]: E0115 22:14:16.988532    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:14:16 minikube kubelet[5591]: E0115 22:14:16.988899    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15546828 maxSize=10485760
Jan 15 22:14:26 minikube kubelet[5591]: E0115 22:14:26.996977    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:14:26 minikube kubelet[5591]: E0115 22:14:26.997260    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15548747 maxSize=10485760
Jan 15 22:14:32 minikube kubelet[5591]: I0115 22:14:32.787870    5591 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"www-data\" (UniqueName: \"kubernetes.io/empty-dir/03f4bf78-b13d-4703-b0d9-339519ce75ec-www-data\") pod \"apache-local-pod\" (UID: \"03f4bf78-b13d-4703-b0d9-339519ce75ec\") " pod="default/apache-local-pod"
Jan 15 22:14:32 minikube kubelet[5591]: I0115 22:14:32.788617    5591 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"php-config\" (UniqueName: \"kubernetes.io/empty-dir/03f4bf78-b13d-4703-b0d9-339519ce75ec-php-config\") pod \"apache-local-pod\" (UID: \"03f4bf78-b13d-4703-b0d9-339519ce75ec\") " pod="default/apache-local-pod"
Jan 15 22:14:32 minikube kubelet[5591]: I0115 22:14:32.788652    5591 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nwvlm\" (UniqueName: \"kubernetes.io/projected/03f4bf78-b13d-4703-b0d9-339519ce75ec-kube-api-access-nwvlm\") pod \"apache-local-pod\" (UID: \"03f4bf78-b13d-4703-b0d9-339519ce75ec\") " pod="default/apache-local-pod"
Jan 15 22:14:33 minikube kubelet[5591]: I0115 22:14:33.410992    5591 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e755c5ff511eb1d6ad212f34c3c7e39e5b5e45aab30e75d037684a2d95a4bce6"
Jan 15 22:14:34 minikube kubelet[5591]: I0115 22:14:34.453177    5591 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/apache-local-pod" podStartSLOduration=2.451844658 podStartE2EDuration="2.451844658s" podCreationTimestamp="2026-01-15 22:14:32 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2026-01-15 22:14:34.451151096 +0000 UTC m=+76998.046212255" watchObservedRunningTime="2026-01-15 22:14:34.451844658 +0000 UTC m=+76998.046905813"
Jan 15 22:14:37 minikube kubelet[5591]: E0115 22:14:37.008536    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:14:37 minikube kubelet[5591]: E0115 22:14:37.008773    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15550667 maxSize=10485760
Jan 15 22:14:47 minikube kubelet[5591]: E0115 22:14:47.013297    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:14:47 minikube kubelet[5591]: E0115 22:14:47.013497    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15552587 maxSize=10485760
Jan 15 22:14:57 minikube kubelet[5591]: E0115 22:14:57.018923    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:14:57 minikube kubelet[5591]: E0115 22:14:57.019032    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15554505 maxSize=10485760
Jan 15 22:15:07 minikube kubelet[5591]: E0115 22:15:07.027506    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:15:07 minikube kubelet[5591]: E0115 22:15:07.027704    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15556422 maxSize=10485760
Jan 15 22:15:17 minikube kubelet[5591]: E0115 22:15:17.032399    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:15:17 minikube kubelet[5591]: E0115 22:15:17.032649    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15558342 maxSize=10485760
Jan 15 22:15:27 minikube kubelet[5591]: E0115 22:15:27.039432    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:15:27 minikube kubelet[5591]: E0115 22:15:27.039591    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15560262 maxSize=10485760
Jan 15 22:15:37 minikube kubelet[5591]: E0115 22:15:37.045495    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:15:37 minikube kubelet[5591]: E0115 22:15:37.045618    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15562181 maxSize=10485760
Jan 15 22:15:47 minikube kubelet[5591]: E0115 22:15:47.049736    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:15:47 minikube kubelet[5591]: E0115 22:15:47.049911    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15564099 maxSize=10485760
Jan 15 22:15:57 minikube kubelet[5591]: E0115 22:15:57.060966    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:15:57 minikube kubelet[5591]: E0115 22:15:57.061162    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15566018 maxSize=10485760
Jan 15 22:16:07 minikube kubelet[5591]: E0115 22:16:07.070133    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:16:07 minikube kubelet[5591]: E0115 22:16:07.070338    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15567935 maxSize=10485760
Jan 15 22:16:17 minikube kubelet[5591]: E0115 22:16:17.068067    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:16:17 minikube kubelet[5591]: E0115 22:16:17.068158    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15569853 maxSize=10485760
Jan 15 22:16:27 minikube kubelet[5591]: E0115 22:16:27.070555    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:16:27 minikube kubelet[5591]: E0115 22:16:27.070850    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15571773 maxSize=10485760
Jan 15 22:16:37 minikube kubelet[5591]: E0115 22:16:37.076100    5591 log.go:32] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117"
Jan 15 22:16:37 minikube kubelet[5591]: E0115 22:16:37.076222    5591 container_log_manager.go:263] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log\": failed to reopen container log \"a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117\": rpc error: code = Unknown desc = docker does not support reopening container log files" worker=1 containerID="a5edb0584fb5660f6404c5e317571d86421af0c7f0e71c9368ec542d8cd4e117" path="/var/log/pods/kube-system_storage-provisioner_6db460be-9810-4b42-b06a-8c74390c5687/storage-provisioner/27.log" currentSize=15573691 maxSize=10485760


==> kubernetes-dashboard [a2ef6ef3eb80] <==
2026/01/14 14:03:22 Using namespace: kubernetes-dashboard
2026/01/14 14:03:22 Using in-cluster config to connect to apiserver
2026/01/14 14:03:22 Using secret token for csrf signing
2026/01/14 14:03:22 Initializing csrf token from kubernetes-dashboard-csrf secret
2026/01/14 14:03:22 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2026/01/14 14:03:22 Successful initial request to the apiserver, version: v1.34.0
2026/01/14 14:03:22 Generating JWE encryption key
2026/01/14 14:03:22 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2026/01/14 14:03:22 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2026/01/14 14:03:22 Initializing JWE encryption key from synchronized object
2026/01/14 14:03:22 Creating in-cluster Sidecar client
2026/01/14 14:03:22 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2026/01/14 14:03:22 Serving insecurely on HTTP port: 9090
2026/01/14 14:03:52 Successful request to sidecar
2026/01/14 14:03:22 Starting overwatch


==> kubernetes-dashboard [cc80e71e4c05] <==
2026/01/14 14:03:16 Using namespace: kubernetes-dashboard
2026/01/14 14:03:16 Using in-cluster config to connect to apiserver
2026/01/14 14:03:16 Using secret token for csrf signing
2026/01/14 14:03:16 Initializing csrf token from kubernetes-dashboard-csrf secret
2026/01/14 14:03:16 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: network is unreachable

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00061fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000162080)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf


==> storage-provisioner [6617e1648ab3] <==
I0114 14:03:12.969827       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0114 14:03:42.989165       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [a5edb0584fb5] <==
W0115 22:15:44.604091       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:44.609582       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:46.612208       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:46.616883       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:48.621143       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:48.628033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:50.629698       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:50.633329       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:52.636733       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:52.644768       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:54.648438       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:54.655608       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:56.659862       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:56.665746       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:58.667917       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:15:58.672182       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:00.676602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:00.683807       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:02.687517       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:02.694243       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:04.699219       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:04.706909       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:06.709741       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:06.714445       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:08.717285       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:08.723179       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:10.726238       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:10.731040       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:12.734849       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:12.742985       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:14.747730       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:14.753726       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:16.756846       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:16.763632       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:18.766563       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:18.771144       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:20.774799       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:20.781479       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:22.784974       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:22.789595       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:24.792422       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:24.796239       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:26.798063       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:26.801713       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:28.804014       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:28.808153       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:30.810707       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:30.814728       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:32.818252       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:32.824195       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:34.827911       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:34.835496       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:36.839632       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:36.845921       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:38.848972       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:38.853410       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:40.856278       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:40.861948       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:42.863698       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0115 22:16:42.867882       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

